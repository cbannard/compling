{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_7KjskLX1q-"
   },
   "source": [
    "# LELA32051 Computational Linguistics Week 9\n",
    "\n",
    "This week we are going to take a look at part of speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcTkkt4bX1rB"
   },
   "source": [
    "## Tagged corpora\n",
    "In looking to understand part of speech tagging, it is useful to start by looking at some human (rather than machine) tagged data. NLTK contains a number of corpora. We can import a few of these as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhYuBQkZX1rB"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "nltk.download('sinica_treebank')\n",
    "nltk.download('indian')\n",
    "nltk.download('conll2002')\n",
    "nltk.download('cess_cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8FhvPh-X7Lo"
   },
   "outputs": [],
   "source": [
    "brown.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qBsMpgZXYWZ1"
   },
   "outputs": [],
   "source": [
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2p4sTYzbYYMu"
   },
   "outputs": [],
   "source": [
    "brown.tagged_words(tagset=\"universal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fOKYUStX1rD"
   },
   "outputs": [],
   "source": [
    "nltk.corpus.sinica_treebank.tagged_words() # Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rL4gHKSX1rD"
   },
   "outputs": [],
   "source": [
    "nltk.corpus.indian.tagged_words() # Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VOXUk5uPX1rE"
   },
   "outputs": [],
   "source": [
    "nltk.corpus.conll2002.tagged_words() # Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yHBp369FX1rF"
   },
   "outputs": [],
   "source": [
    "nltk.corpus.cess_cat.tagged_words() # Catalan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sS4HRV5PaJ7p"
   },
   "source": [
    "## Inspecting tagged corpora\n",
    "\n",
    "Inspecting human tagged corpora can be useful for both linguistic research and for building taggers. To look at how we can do this I'll introduce another Python basic (the last of the semester)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zu4T6ySfX1rJ"
   },
   "source": [
    "### Dictionaries\n",
    "\n",
    "A data structure that we are making use of here that is very useful in Python is the Dictionary. This a structure that allows you to store a look up associations between \"key\" and \"values\".\n",
    "\n",
    "An empty dictionary is created by writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1kKw3skQX1rJ"
   },
   "outputs": [],
   "source": [
    "pos = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbcH4rKmX1rK"
   },
   "source": [
    "You can then add entries to it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gnvIL4s4X1rK"
   },
   "outputs": [],
   "source": [
    "pos['colourless'] = 'ADJ'\n",
    "pos['ideas'] = 'NOUN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWqsgAJEncJ7"
   },
   "source": [
    "The full dictionary can be viewed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zimxoszOnen5"
   },
   "outputs": [],
   "source": [
    "print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0v0dxdWX1rK"
   },
   "source": [
    "And you can look up entries by key as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTECj6PIX1rK"
   },
   "outputs": [],
   "source": [
    "pos['colourless']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data structure is very useful in performing quantitative analysis of tagged corpora, as it can be used to make and store counts. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = [(\"the\",\"DET\"),(\"man\",\"NOUN\"),(\"walked\",\"VERB\"),(\"the\",\"DET\"),(\"dog\",\"NOUN\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = dict.fromkeys(sent,0)\n",
    "for s in sent:\n",
    "    wt[s] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK actually provides a special data structure that does this for us more efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.FreqDist(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some purposes we want to create dictionaries in which the values themselves are dictionaries. So for example we might have a dictionary in which a word is the key and the value is a dictionary in which the keys are the POS tags that occur in with that word and the values are the counts for each of these POS tags for that word.\n",
    "\n",
    "NLTK again provides a nice implementation of a nested dictionary structure for us, the ConditionalFreqDist. We can populate this ourselves like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import ConditionalFreqDist\n",
    "cfd = ConditionalFreqDist()\n",
    "for w,t in sent:\n",
    "    cfd[w][t] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd['dog']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can let NLTK do the work for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd1 = nltk.ConditionalFreqDist(sent)\n",
    "cfd1['dog']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9MY9J0JO8ZU"
   },
   "source": [
    "When we apply this to whole corpora, it becomes very useful.\n",
    "\n",
    "Most straightforwardly we can look (as we have above for the single sentence) at the frequency with which particular words are given a tag (we will return to this later when we come to build a tagger)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3reX-cyOO8ZU"
   },
   "outputs": [],
   "source": [
    "brown_tagged = brown.tagged_words(tagset='universal')\n",
    "cfd1 = nltk.ConditionalFreqDist(brown_tagged)\n",
    "cfd1['walk']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgPMnoS7O8ZV"
   },
   "source": [
    "And if we additionally use a couple of other NLTK tools (which we don't have time to cover - I just want to give you a sense of what is possible), we can look at the frequency with which particular word classes precede particular words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qOYrY5lbX1rF"
   },
   "outputs": [],
   "source": [
    "brown_tagged = brown.tagged_words(tagset='universal')\n",
    "tags = [b[1] for (a, b) in nltk.bigrams(brown_tagged) if a[0] == 'often']\n",
    "fd = nltk.FreqDist(tags)\n",
    "fd.tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPvjkxfDX1rH"
   },
   "source": [
    "Or the frequency with which particular word classes precede other word classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8xvG9WOAX1rH"
   },
   "outputs": [],
   "source": [
    "brown_tagged = brown.tagged_words(tagset='universal')\n",
    "word_tag_pairs = nltk.bigrams(brown_tagged)\n",
    "noun_preceders = [a[1] for (a, b) in word_tag_pairs if b[1] == 'NOUN']\n",
    "noun_preceders_fd = nltk.FreqDist(noun_preceders)\n",
    "[(wt,_) for (wt, _) in noun_preceders_fd.most_common()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XdK3VjNO8ZW"
   },
   "source": [
    "And you can even search for particular constructional patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ulo7vBehX1rI"
   },
   "outputs": [],
   "source": [
    "for tagged_sent in brown.tagged_sents(categories=\"news\"):\n",
    "    for (w1,t1), (w2,t2), (w3,t3) in nltk.trigrams(tagged_sent):\n",
    "        if (t1.startswith('V') and w2 == 'to' and t3.startswith('V')):\n",
    "            print(w1, w2, w3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6ib7GseX1rK"
   },
   "source": [
    "## Building a tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVqeMRw0O8ZW"
   },
   "source": [
    "In week three we looked at vector space models and saw that just by looking at the words that surround other words we can group them together into something like word classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBlQ6-gFO8ZW"
   },
   "outputs": [],
   "source": [
    "text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())\n",
    "text.similar('car')\n",
    "text.similar('the')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzA8ISaKO8ZW"
   },
   "source": [
    "However this doesn't give great performance, and it is more common to build taggers by taking human-tagged data and learning to classify words by using the words and tags as training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3XT2SiLO8ZW"
   },
   "source": [
    "A very simple approach that actually works quite well is to find the most common tag for each word in a training corpus (as we did above) and just tag all occurences of each word with its most common tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RhEZn32bX1rL"
   },
   "outputs": [],
   "source": [
    "brown_tagged_sents = brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5kVAmnCIX1rM"
   },
   "outputs": [],
   "source": [
    "unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mF1KCV3gX1rM"
   },
   "outputs": [],
   "source": [
    "unigram_tagger.tag([\"the\",\"cat\",\"sat\",\"on\",\"the\",\"mat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guYS2yoPO8ZX"
   },
   "source": [
    "We can formally evaluate this by splitting our data into a training set and a testing set. We obtain the by-word tag frequencies from the training set and evaluate by tagging the test set and comparing our predicted tags to the human tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fezj3MKPX1rM"
   },
   "outputs": [],
   "source": [
    "training_set_size = int(len(brown_tagged_sents) * 0.9)\n",
    "train_sents = brown_tagged_sents[:training_set_size]\n",
    "test_sents = brown_tagged_sents[training_set_size:]\n",
    "unigram_tagger = nltk.UnigramTagger(train_sents)\n",
    "unigram_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ft2V8G1DO8ZX"
   },
   "source": [
    "We want to improve this, and an obvious next step is to give the tag that is most frequent for this word when it follows the previous word. The problem is this doesn't do very well. Any idea why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpyhojpNX1rM"
   },
   "outputs": [],
   "source": [
    "bigram_tagger = nltk.BigramTagger(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4PK9DIBX1rN"
   },
   "outputs": [],
   "source": [
    "bigram_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThUxUTrqO8ZY"
   },
   "source": [
    "We can still make use of the bigram information by combining it with the unigram tagger via a process known as backing off - for each word we check whether we have seen that word and preceding word in our training data. If we have then we tag it with the most frequent tag for that word in that context. If we haven't seen it then we tag the word with its most frequent tag regardless of context. And if we haven't seen the word before we tag it as a noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QRIRmxiIX1rN"
   },
   "outputs": [],
   "source": [
    "t0 = nltk.DefaultTagger('NOUN')\n",
    "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "t2.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aulYA40rX1rL"
   },
   "source": [
    "### Regular expression based tagging\n",
    "\n",
    "As a next step we want to use a more intelligent way to deal with words we haven't seen before, but making use of their orthography and/or morphology. Write regular expressions to classify words in this way and see if you can improve performance. I've added one example rule to get you started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-xWlo8jX1rL"
   },
   "outputs": [],
   "source": [
    "patterns = [\n",
    "    (r'.*ing$', 'VERB'),\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HL5p4DXGX1rN"
   },
   "outputs": [],
   "source": [
    "t0 = nltk.DefaultTagger('NOUN')\n",
    "t1 = nltk.RegexpTagger(patterns, backoff=t0)\n",
    "t2 = nltk.UnigramTagger(train_sents, backoff=t1)\n",
    "t3 = nltk.BigramTagger(train_sents, backoff=t2)\n",
    "t3.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XW49NJtgO8ZZ"
   },
   "source": [
    "As with other classification tasks we can generate a confusion matrix to see where things are going right or wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SXMlkijYX1rN"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "predicted = [tag for sent in brown.sents(categories='editorial') for (word, tag) in t3.tag(sent)]\n",
    "true = [tag for (word, tag) in brown.tagged_words(categories='editorial',tagset=\"universal\")]\n",
    "print(pd.DataFrame(confusion_matrix(predicted, true),index=set(predicted)))         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99NYaf-rX1rN"
   },
   "source": [
    "### NLTK's Averaged Perceptron tagger\n",
    "\n",
    "NLTKs default prebuilt tagger uses a Perceptron just like that we have been using for other tasks on the module. For more information on this approach see here: https://explosion.ai/blog/part-of-speech-pos-tagger-in-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cARyYZbeumrs"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0ivT2SRurCm"
   },
   "source": [
    "It can be run straightforwardly like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fAZY4tcwX1rN"
   },
   "outputs": [],
   "source": [
    "text = nltk.word_tokenize(\"And now for something completely different\")\n",
    "nltk.pos_tag(text, tagset=\"universal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXR-hDjnX1rN"
   },
   "source": [
    "### POS tagging in other languages\n",
    "\n",
    "POS taggers are available for a great many languages. A popular package called Spacy contains a number. Here, as an example, is a German tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 13355,
     "status": "ok",
     "timestamp": 1637879925369,
     "user": {
      "displayName": "Colin Bannard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3NcIkMWzsy6zxo7AXqwviYA_hgjuzVq_zKWseQ=s64",
      "userId": "01716265927303848317"
     },
     "user_tz": 0
    },
    "id": "zegtIf97Pvwg",
    "outputId": "71137cdf-2399-4423-947d-d803468bc06c"
   },
   "outputs": [],
   "source": [
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9036,
     "status": "ok",
     "timestamp": 1637879961287,
     "user": {
      "displayName": "Colin Bannard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3NcIkMWzsy6zxo7AXqwviYA_hgjuzVq_zKWseQ=s64",
      "userId": "01716265927303848317"
     },
     "user_tz": 0
    },
    "id": "Ltbco6s5QJD8"
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11045,
     "status": "ok",
     "timestamp": 1637879974505,
     "user": {
      "displayName": "Colin Bannard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3NcIkMWzsy6zxo7AXqwviYA_hgjuzVq_zKWseQ=s64",
      "userId": "01716265927303848317"
     },
     "user_tz": 0
    },
    "id": "sR6hNMiiX1rO",
    "outputId": "7980ec2b-42d4-489e-8be5-c8fab28a9d0b"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2807,
     "status": "ok",
     "timestamp": 1637879982231,
     "user": {
      "displayName": "Colin Bannard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3NcIkMWzsy6zxo7AXqwviYA_hgjuzVq_zKWseQ=s64",
      "userId": "01716265927303848317"
     },
     "user_tz": 0
    },
    "id": "vAAtX7qNX1rO"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('de_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 215,
     "status": "ok",
     "timestamp": 1637880094683,
     "user": {
      "displayName": "Colin Bannard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3NcIkMWzsy6zxo7AXqwviYA_hgjuzVq_zKWseQ=s64",
      "userId": "01716265927303848317"
     },
     "user_tz": 0
    },
    "id": "Qtrht5kAQocj"
   },
   "outputs": [],
   "source": [
    "text = \"Das ist nicht gut.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 362,
     "status": "ok",
     "timestamp": 1637880096535,
     "user": {
      "displayName": "Colin Bannard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3NcIkMWzsy6zxo7AXqwviYA_hgjuzVq_zKWseQ=s64",
      "userId": "01716265927303848317"
     },
     "user_tz": 0
    },
    "id": "7jQcjD8PX1rO"
   },
   "outputs": [],
   "source": [
    "s1_t = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 229,
     "status": "ok",
     "timestamp": 1637880100637,
     "user": {
      "displayName": "Colin Bannard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3NcIkMWzsy6zxo7AXqwviYA_hgjuzVq_zKWseQ=s64",
      "userId": "01716265927303848317"
     },
     "user_tz": 0
    },
    "id": "QuC66AL2X1rO",
    "outputId": "3596dd2e-7c30-42fd-dff7-44e45091fb3d"
   },
   "outputs": [],
   "source": [
    "for tk in s1_t:\n",
    "    print(tk.text, tk.tag_, tk.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqQ6hQEziFSp"
   },
   "source": [
    "### Chunking / Shallow Parsing\n",
    "\n",
    "Chunking involves grouping together words into elementary phrases. In its most common form it doesn't involve any hierachical structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iU7Nm8nxWtP3"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvmAzVJnh4It"
   },
   "outputs": [],
   "source": [
    "text = nltk.word_tokenize(\"I study Linguistics and Social Anthropology at the University of Manchester\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5zDIdnUEWIFJ"
   },
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "  NP: {<DET|ADP>?<ADJ>*<NOUN>}\n",
    "      {<NOUN>+} \n",
    "\"\"\"\n",
    "sent=nltk.pos_tag(text,tagset=\"universal\")\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "cs = cp.parse(sent)\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e53dKlaEg73e"
   },
   "source": [
    "Update the grammar so that it produces the following shallow parse: <br> <br>\n",
    "(S <br>\n",
    "  (NP I/PRON) <br>\n",
    "  study/VERB <br>\n",
    "  (NP Linguistics/NOUN and/CONJ Social/NOUN Anthropology/NOUN) <br>\n",
    "  at/ADP <br>\n",
    "  (NP the/DET University/NOUN of/ADP Manchester/NOUN)) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmMIKn0HWJwv"
   },
   "source": [
    "### Named Entity Recognition\n",
    "\n",
    "NLTK also has an inbuilt named entity recognition tool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykpXHBu7kb1X"
   },
   "outputs": [],
   "source": [
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1mHPnO1YF3J"
   },
   "outputs": [],
   "source": [
    "sent=nltk.pos_tag(text,tagset=\"universal\")\n",
    "print(nltk.ne_chunk(sent))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week_9_Seminar.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
