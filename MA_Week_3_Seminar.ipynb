{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ada7675f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LELA70331 Computational Linguistics Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c58486",
   "metadata": {},
   "source": [
    "## Regular Expressions\n",
    "\n",
    "In this week's session we are going to look first at a tool that is crucial in implementing the preprocessing tasks introduced in the lecture - regular expressions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f6fcae",
   "metadata": {},
   "source": [
    "In week 1 we used functions that belong to the datatype string to manipulate text. This week we are going to explore a much more powerful way of manipulating text - the regular expression. In order to do this we need to import the regular expressions library (https://docs.python.org/3/library/re.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4e577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add0aad8",
   "metadata": {},
   "source": [
    "In order to explore this we will import text again and tokenise it in the same way we did last week. However, instead of downloading the file and uploading it again we are going to use a tool call wget to retrieve the online file directly from Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af7bbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.gutenberg.org/files/2554/2554-0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d285d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('2554-0.txt')\n",
    "raw = f.read()\n",
    "chapter_one = raw[5464:23725]\n",
    "chapter_one = chapter_one.replace(\".\",\" .\")\n",
    "chapter_one_tokens = str.split(chapter_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9a9432-87c1-4ddf-9f55-b50c88500410",
   "metadata": {},
   "source": [
    "### re.search()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ea0e59",
   "metadata": {},
   "source": [
    "There are a few functions in the re library that we will need to learn about. One of these is re.search - this searches for occurence of a pattern in a given string. It takes a regular expression to search for, between quotes, as its first argument and a string to search as its second argument. It returns a boolean value of true (actually a match object with a boolean value of true but we can ignore that nuance) if it finds an occurence of the pattern. We can use it to search through a list of tokens and print out tokens that match a target pattern using a for loop and an if statement as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a24651",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in chapter_one_tokens:\n",
    "    if re.search(\"ed\", w):\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eab04e1-81ce-44cb-93d6-b7a9ae13d7a9",
   "metadata": {},
   "source": [
    "We can use search to try out a couple of aspects of regular expressions we learned about in the week 2 lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2a3b40-d007-4fcc-987b-9672ba809d62",
   "metadata": {},
   "source": [
    "Activity: Alter the regular expression so that it will detect sequences that contain ed or er."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec19b76-62ac-4626-a7bc-5320ce600d06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee53232c-8821-4893-8f7f-44541fbed2eb",
   "metadata": {},
   "source": [
    "Activity: Alter the regular expression so that it will detect any single character followed by the letter \"d\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c00ee9-ca07-486e-9183-7886ea555155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ae61ef4-4fbc-4df2-80ac-a890d33823b0",
   "metadata": {},
   "source": [
    "Activity: Alter the regular expression so that it will detect any sequence of one of more character followed by the letter \"d\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a4d305-d779-489c-92b1-77b78bb08e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f451186-a838-45c4-b5b5-239e65f8c917",
   "metadata": {},
   "source": [
    "Activity: Alter the regular expression so that it will detect any string that contains any letter other than \"e\" followed by \"d\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feff6eb-4d0b-42c0-b3e6-a5b5f54d1958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99c3dc1f-4e63-450c-a54c-04bb9c498254",
   "metadata": {},
   "source": [
    "This brings us to the first technique that wasn't covered in the lecture. A dollar sign marks the end of the string being searched, so if we put it at the end of our pattern it will only find patterns that occur at the end of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8632e410-0a7c-4500-9dc3-9191c20ad08b",
   "metadata": {},
   "source": [
    "Activity: update the loop below so that it only matches tokens that end in \"er\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d654f7-1324-491f-a562-c5aa6f9b6ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in chapter_one_tokens:\n",
    "    if re.search(\"er\", w):\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ab42c9-7c82-4a21-8944-c53d4193106e",
   "metadata": {},
   "source": [
    "A carat symbol marks the beginning of a string so can be used to make sure the pattern is only matched when it occurs at the beginning of the string being searched."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ad492a-d68b-44e3-9be6-6b154088d067",
   "metadata": {},
   "source": [
    "Activity: Create a loop below so that it matches any tokens that begin with b or B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c43e2e-a9b8-4f90-b73f-180f806df95b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78d69e77-ca73-40e8-94ca-5ff1b5b65d2b",
   "metadata": {},
   "source": [
    "### re.findall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075081ea-11b2-4ecb-81fc-87f38fce17b0",
   "metadata": {},
   "source": [
    "Another useful function is re.findall. This searches for patterns and returns all substrings that are matched by the pattern:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554e9792-1b77-4165-a9c5-dc0f5706df1b",
   "metadata": {},
   "source": [
    "re.findall will return ALL patterns in a string so we can actually run it on non-tokenised text and dispense with the for loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e618e8f8-dbab-470c-8451-2631f3a0b28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lodged', 'walked', 'avoided', 'five-storied', 'provided', 'lived', 'obliged', 'passed', 'frightened', 'ashamed', 'overstrained', 'absorbed', 'isolated', 'dreaded', 'crushed', 'ceased', 'stopped', 'forced', 'frightened', 'learned', 'worked', 'completed', 'gleamed', 'refined', 'walked', 'confessed', 'tasted', 'dressed', 'accustomed', 'ashamed', 'have\\ncreated', 'crowded', 'caused', 'accumulated', 'minded', 'indeed', 'disliked', 'dragged', 'shouted', 'stopped', 'clutched', 'bespattered', 'muttered', 'noticed', 'remembered', 'indeed', 'hundred', 'counted', 'jeered', 'attempted', 'looked', 'inhabited', 'employed', 'slipped', 'unnoticed', 'liked', 'dreaded', 'scared', 'he\\nreached', 'barred', 'engaged', 'occupied', 'untenanted', 'seemed', 'started', 'overstrained', 'opened', 'eyed', 'and\\nopened', 'stepped', 'partitioned', 'diminutive,\\nwithered', 'grizzled', 'smeared', 'looked', 'knotted', 'coughed', 'groaned', 'looked', 'continued', 'disconcerted', 'surprised', 'paused', 'stepped', 'walked', 'brightly\\nlighted', 'flashed', 'scanned', 'consisted', 'fixed', 'polished', 'bed', 'looked', 'old-fashioned', 'engraved', 'pled', 'pled', 'red', 'cried', 'handed', 'checked', 'fumbled', 'disappeared', 'listened', 'reflected', 'looked', 'wanted', 'asked', 'asked', 'stopped', 'cried', 'added', 'reached', 'wretched', 'walked', 'noticed', 'entered', 'mounted', 'tormented', 'longed', 'attributed', 'ordered', 'gazed', 'appeared', 'full-skirted', 'had\\ndropped', 'hummed', 'loved', 'loved', 'crowded', 'used', 'shared', 'looked', 'looked', 'retired', 'appeared']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(\"[^ ]+ed\", chapter_one))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd639fea-ee32-420b-9dc9-3eeebaf760d8",
   "metadata": {},
   "source": [
    "New function: len() is a built-in Python function that tells use the length of various data types and structures including strings and lists: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00965e09-5ce8-4d85-aaff-1aca7109ffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chapter_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284f1d30-c54e-4cb6-9020-95c087ee6d93",
   "metadata": {},
   "source": [
    "Activity: Use the len function together with findall in order to count occurences of word containing the sequence \"ed\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4f6987-d22a-48ff-b837-dee4153cca27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "559c5b61-2875-4c5e-805e-064b0e7311bb",
   "metadata": {},
   "source": [
    "### re.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3610cb9f-f6ea-4ab3-8ad9-59c225502a84",
   "metadata": {},
   "source": [
    "For patterns that we want to use again and again we can use the function re.compile(). This returns a re \"object\" that has all of the re functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a557eb5-5b56-4f90-9fde-f65e8e30200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_tense = re.compile(\"ed\")\n",
    "print(past_tense.findall(chapter_one))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5397d1df-cb85-45b0-b02d-3eda67c2863c",
   "metadata": {},
   "source": [
    "### Escaping special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e158957-aa5e-497b-837e-bc42a74f263f",
   "metadata": {},
   "source": [
    "We have learned about a number of character that have a special meaning in regular expressions (periods, dollar signs etc). We might sometimes want to search for these characters in strings. To do this we can \"escape\" the character using a backslash(\\) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f56db94-2465-462b-a01a-0d1d18918c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(\"\\.\",chapter_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c2c1c7-993d-4338-97dd-d3c33c49cc4f",
   "metadata": {},
   "source": [
    "### re.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ba8578-45c9-498d-be68-5c6dc4af2c18",
   "metadata": {},
   "source": [
    "re.split() takes a regular expression as a first argument (unless you have a precompiled pattern) and a string as second argument, and split the string into tokens divided by all substrings matched by the regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c924581e-f6a3-4aee-85f2-b057d521e944",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_split_on = re.compile(\" \")\n",
    "chapter_one_tokens_new = to_split_on.split(chapter_one)\n",
    "print(chapter_one_tokens_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67651269-ac9b-4956-bcbe-20f54e705108",
   "metadata": {},
   "source": [
    "## Putting it all together: Some exercises to try in your own time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f56205-c74b-430c-b9b1-a4f362698ad1",
   "metadata": {},
   "source": [
    "Activity: Can we use the functions and techniques we have learned about so far in order to build a functioning tokenizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfe3cac-3e8e-49a1-a7ea-43a5d9fe9ef7",
   "metadata": {},
   "source": [
    "The code below is a simple solution but it has a number of problems as discussed in the lecture. Can you use regular expressions to overcome these problems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8971495-514e-4c75-87a1-d2cc4ad0ac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_split_on = re.compile(\" \")\n",
    "chapter_one_tokens_new = to_split_on.split(chapter_one)\n",
    "print(chapter_one_tokens_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6b6a1a-4264-4997-bfe0-09d5b7fb94b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a8e6896-0712-491e-9343-518b186cb3b2",
   "metadata": {},
   "source": [
    "Activity: Can we use the functions and techniques we have learned about so far in order to build a functioning sentence segmenter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3946d476-6fdd-46e4-937c-2cbef76ab5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e06cf589-4bd6-4803-96cf-ed86a34d5011",
   "metadata": {},
   "source": [
    "Activity: Can we use the functions and techniques we have learned about so far in order to build a functioning lemmatizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58086b07-69d9-4f58-ae04-acb2d5587255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "904101cb-91ef-47ab-b918-a7ba7e743bf5",
   "metadata": {
    "id": "904101cb-91ef-47ab-b918-a7ba7e743bf5"
   },
   "source": [
    "# Vector semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8Zj1DoDsYQQV",
   "metadata": {
    "id": "8Zj1DoDsYQQV"
   },
   "source": [
    "In this week's lecture you heard about Vector-based semantics. Today we will take a look at these models in Python. First we will build a co-occurence model from the now very familiar Crime and Punishment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de36ce0b-40cb-4670-ae95-95056251eacc",
   "metadata": {},
   "source": [
    "First we will segment and tokenize the whole novel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ddd30f-fb1f-49f8-b10e-6411e2d20221",
   "metadata": {},
   "source": [
    "So far we have looked at the core Python programming language and the re library. However much of the time this semester we will be making use of even more  powerful libraries for natural language processing and machine learning. Today we will make use of a few of these. The first of these is \"Natural Language Toolkit\" or nltk (http://www.nltk.org/).\n",
    "\n",
    "The first thing we need to do is to make sure we have the libraries we want installed. On Google Colab they are all already there. If your are using your own machine you will have to install it using the following command (unlike for re which is present by default and just needs to be loaded)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2d321e-3cc2-49a5-bb14-ef0b7134fe31",
   "metadata": {},
   "source": [
    "!pip install nltk # If using anaconda convert this to a code window and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750cd9bc-c1be-4e45-b6bd-d7ae03453ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395249e2-e086-45a2-9856-535e1671e808",
   "metadata": {},
   "source": [
    "nltk provides us with a tokenizer and sentence segmenter that work pretty well. We can combine them to give us what we need to build word vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486440c1-da92-46fb-99fe-ee91accfd87c",
   "metadata": {
    "executionInfo": {
     "elapsed": 2813,
     "status": "ok",
     "timestamp": 1634141036095,
     "user": {
      "displayName": "Colin Bannard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3NcIkMWzsy6zxo7AXqwviYA_hgjuzVq_zKWseQ=s64",
      "userId": "01716265927303848317"
     },
     "user_tz": -60
    },
    "id": "486440c1-da92-46fb-99fe-ee91accfd87c"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "C_and_P_tokens_sentences = []\n",
    "for sent in nltk.sent_tokenize(raw):\n",
    "    C_and_P_tokens_sentences.append(nltk.word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0965bfb9-8cab-445c-b131-b5735c028b68",
   "metadata": {},
   "source": [
    "Next we will build a cooccurence matrix using the following function. The purpose of this is to aid your conceptual understanding by looking at the output, and you aren't expected to read or understand this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b0cd3e-8756-40ac-954e-46a54f09b597",
   "metadata": {
    "executionInfo": {
     "elapsed": 3249,
     "status": "ok",
     "timestamp": 1634141089200,
     "user": {
      "displayName": "Colin Bannard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3NcIkMWzsy6zxo7AXqwviYA_hgjuzVq_zKWseQ=s64",
      "userId": "01716265927303848317"
     },
     "user_tz": -60
    },
    "id": "b2b0cd3e-8756-40ac-954e-46a54f09b597"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function from https://aegis4048.github.io/understanding_multi-dimensionality_in_vector_space_modeling\n",
    "def compute_co_occurrence_matrix(corpus, window_size=4): \n",
    "    \n",
    "    # Get a sorted list of all vocab items\n",
    "    distinct_words = sorted(list(set([word for sentence in corpus for word in sentence])))\n",
    "    # Find vocabulary size\n",
    "    num_words = len(distinct_words)\n",
    "    # Create a Word Dictionary mapping each word to a unique index\n",
    "    word2Ind = {word: index for index, word in enumerate(distinct_words)}\n",
    "    \n",
    "    # Create a numpy matrix in order to store co-occurence counts\n",
    "    M = np.zeros((num_words, num_words))\n",
    "\n",
    "    # Iterate over sentences in text\n",
    "    for sentence in corpus:\n",
    "        # Iterate over words in each sentence\n",
    "        for i, word in enumerate(sentence):      \n",
    "            # Find the index in the tokenized sentence vector for the beginning of the window (the current token minus window size or zero whichever is the lower)\n",
    "            begin = max(i - window_size, 0)\n",
    "            # Find the index in the tokenized sentence vector for the end of the window (the current token plus window size or the length of the sentence whichever is the lower)\n",
    "            end   = min(i + window_size, num_words)\n",
    "            # Extract the text from beginning of window to the end\n",
    "            context = sentence[begin: end + 1]\n",
    "            # Remove the target word from its own window \n",
    "            context.remove(sentence[i])\n",
    "            # Find the row for the current target word  \n",
    "            current_row = word2Ind[word]\n",
    "            # Iterate over the window for this target word\n",
    "            for token in context:\n",
    "                # Find the ID and hence the column index for the current token \n",
    "                current_col = word2Ind[token]\n",
    "                # Add 1 to the current context word dimension for the current target word\n",
    "                M[current_row, current_col] += 1\n",
    "    # Return the co-occurence matrix and the vocabulary to index \"dictionary\"\n",
    "    return M, word2Ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a803921-b41d-482e-afac-86e6434672e5",
   "metadata": {},
   "source": [
    "This function allows us to specify the window that we use as context. We will use a window size of 5 words either side of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803b1b51-5c1c-4090-ae2e-083011a44ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_co_occurrence, word2Ind_co_occurrence = compute_co_occurrence_matrix(C_and_P_tokens_sentences, window_size=5)\n",
    "\n",
    "semantic_space = pd.DataFrame(M_co_occurrence, index=word2Ind_co_occurrence.keys(), columns=word2Ind_co_occurrence.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9475be-3e0e-4705-b814-75e59455b11b",
   "metadata": {},
   "source": [
    "We can look at the size of the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e37e6-f1af-4606-b386-4a15bfffb333",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_space.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d389f36b-2292-40f0-9edd-961e215d8f7d",
   "metadata": {},
   "source": [
    "We can look at a part of the semantic space like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438e23db-cb06-4493-831f-069d552b9f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_space.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555e8642-293f-4b37-a3d4-20b3e0ad89bf",
   "metadata": {},
   "source": [
    "And another example part like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480e5de6-2a00-4596-b8d9-970322c8d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_space.iloc[200:220,200:220]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb763649-0593-41c7-b5a3-40ab9362dcbd",
   "metadata": {},
   "source": [
    "### Dimensionality reduction\n",
    "\n",
    "One thing you will notice is that even for this one text the matrix is enormous, and that it consists mostly of zeros.\n",
    "This makes them difficult to manage. We can address the first issue by just restricting the dimensions using a regular expression: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a8f6df-23ce-44c2-8b9c-f732d4b14d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_space.filter(regex='[a-zA-Z]',axis=0).filter(regex='[a-zA-z]',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16567897-80ed-402d-883b-fd20afe7d694",
   "metadata": {},
   "source": [
    "We could also consider preprocessing the input text to e.g. make all words lower case, or lemmatizing so different forms of the same word are treated as the same dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d4e44e-df70-4800-9624-515f1f0f86fc",
   "metadata": {},
   "source": [
    "None of this however will reduce the number of zeros. What we want is to acquire a \"dense\" rather than a \"sparse\" matrix.\n",
    "\n",
    "There are two popular ways of doing this:\n",
    "\n",
    "1. Produce a count matrix and then compress it via a process known as dimensionality reduction (A method called \"Singular Value Decomposition\" is behind a popular approach known as \"latent semantic analysis\"). \n",
    "2. Learn dense vectors directly using neural networks (this is the best performing method but we'll not cover it yet as we've not learned about neural networks yet).\n",
    "\n",
    "As well as being easier to manage, dense vectors do better at capturing “second order” relations. Car and automobile are synonyms but they are distinct dimensions. A word with car as a neighbor and a word with automobile as a neighbor should be similar, but they aren't.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948f663b-d220-4532-9758-7048c313b730",
   "metadata": {},
   "source": [
    "!pip install -U scikit-learn  # If using anaconda convert this to a code window and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287df5c7-b410-4cf0-b362-f9209b7066d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "def reduce_to_k_dim(M, n_components=2):\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=n_components, n_iter=10, random_state=42)\n",
    "    M_reduced = svd.fit_transform(M_co_occurrence)  \n",
    "    \n",
    "    print('n_components =', n_components)\n",
    "    print('Explained Variance =', round(svd.explained_variance_ratio_.sum(), 3))\n",
    "    \n",
    "    return M_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfa6889-c5d5-42f2-8b44-39f9b8561e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_reduced = reduce_to_k_dim(M_co_occurrence, n_components=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7310412-a3e4-45c2-96d9-ca25e0e2dba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_space = pd.DataFrame(M_reduced, index=word2Ind_co_occurrence.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90fb71b-a910-43af-a9b9-b4132bde1749",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b1cad5-60f2-4551-a67d-fa5b30117dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_space.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ca3055-2046-40fe-a198-6dbf342d6bd8",
   "metadata": {
    "id": "83ca3055-2046-40fe-a198-6dbf342d6bd8"
   },
   "source": [
    "### Saving our vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd50c409-0787-4bc2-9046-d93c90d93dbf",
   "metadata": {
    "executionInfo": {
     "elapsed": 345,
     "status": "ok",
     "timestamp": 1634141102502,
     "user": {
      "displayName": "Colin Bannard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3NcIkMWzsy6zxo7AXqwviYA_hgjuzVq_zKWseQ=s64",
      "userId": "01716265927303848317"
     },
     "user_tz": -60
    },
    "id": "dd50c409-0787-4bc2-9046-d93c90d93dbf"
   },
   "outputs": [],
   "source": [
    "semantic_space.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfde4f53-812b-4073-b548-a29956872f95",
   "metadata": {
    "executionInfo": {
     "elapsed": 90396,
     "status": "ok",
     "timestamp": 1634141194778,
     "user": {
      "displayName": "Colin Bannard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3NcIkMWzsy6zxo7AXqwviYA_hgjuzVq_zKWseQ=s64",
      "userId": "01716265927303848317"
     },
     "user_tz": -60
    },
    "id": "bfde4f53-812b-4073-b548-a29956872f95"
   },
   "outputs": [],
   "source": [
    "np.savetxt(r'np.txt', semantic_space.values,fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4a9307-970d-4616-a331-8562959f7d47",
   "metadata": {
    "id": "fd4a9307-970d-4616-a331-8562959f7d47"
   },
   "source": [
    "# Using our Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048fcb05-e7bb-45d6-903d-e37fe8b61572",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6535,
     "status": "ok",
     "timestamp": 1634141205077,
     "user": {
      "displayName": "Colin Bannard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3NcIkMWzsy6zxo7AXqwviYA_hgjuzVq_zKWseQ=s64",
      "userId": "01716265927303848317"
     },
     "user_tz": -60
    },
    "id": "048fcb05-e7bb-45d6-903d-e37fe8b61572",
    "outputId": "210cb72b-649c-4d2c-d3c4-d1db0fe6d359"
   },
   "outputs": [],
   "source": [
    "!pip install annoy\n",
    "!pip install torch torchvision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969aeb9c-3268-4fd9-9b17-12d3d5cff87e",
   "metadata": {
    "executionInfo": {
     "elapsed": 466,
     "status": "ok",
     "timestamp": 1634141209545,
     "user": {
      "displayName": "Colin Bannard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3NcIkMWzsy6zxo7AXqwviYA_hgjuzVq_zKWseQ=s64",
      "userId": "01716265927303848317"
     },
     "user_tz": -60
    },
    "id": "969aeb9c-3268-4fd9-9b17-12d3d5cff87e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from annoy import AnnoyIndex\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35656cef-b4d1-49c2-a399-91133a7b8fce",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1634141211557,
     "user": {
      "displayName": "Colin Bannard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3NcIkMWzsy6zxo7AXqwviYA_hgjuzVq_zKWseQ=s64",
      "userId": "01716265927303848317"
     },
     "user_tz": -60
    },
    "id": "35656cef-b4d1-49c2-a399-91133a7b8fce"
   },
   "outputs": [],
   "source": [
    "# Function from Rao, D., & McMahan, B. (2019). Natural language processing with PyTorch: build intelligent language applications using deep learning. \" O'Reilly Media, Inc.\".\n",
    "class EmbeddingUtil(object):\n",
    "    \"\"\" A wrapper around pre-trained word vectors and their use \"\"\"\n",
    "    def __init__(self, word_to_index, word_vectors):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            word_to_index (dict): mapping from word to integers\n",
    "            word_vectors (list of numpy arrays)\n",
    "        \"\"\"\n",
    "        self.word_to_index = word_to_index\n",
    "        self.word_vectors = word_vectors\n",
    "        self.index_to_word = {v: k for k, v in self.word_to_index.items()}\n",
    "\n",
    "        self.index = AnnoyIndex(len(word_vectors[0]), metric='angular')\n",
    "        print(\"Building Index!\")\n",
    "        for _, i in self.word_to_index.items():\n",
    "            self.index.add_item(i, self.word_vectors[i])\n",
    "        self.index.build(50)\n",
    "        print(\"Finished!\")\n",
    "        \n",
    "    @classmethod\n",
    "    def from_embeddings_file(cls, embedding_file):\n",
    "        \"\"\"Instantiate from pre-trained vector file.\n",
    "        \n",
    "        Vector file should be of the format:\n",
    "            word0 x0_0 x0_1 x0_2 x0_3 ... x0_N\n",
    "            word1 x1_0 x1_1 x1_2 x1_3 ... x1_N\n",
    "        \n",
    "        Args:\n",
    "            embedding_file (str): location of the file\n",
    "        Returns: \n",
    "            instance of PretrainedEmbeddigns\n",
    "        \"\"\"\n",
    "        word_to_index = {}\n",
    "        word_vectors = []\n",
    "\n",
    "        with open(embedding_file) as fp:\n",
    "            for line in fp.readlines():\n",
    "                line = line.split(\" \")\n",
    "                word = line[0]\n",
    "                vec = np.array([float(x) for x in line[1:]])\n",
    "                \n",
    "                word_to_index[word] = len(word_to_index)\n",
    "                word_vectors.append(vec)\n",
    "                \n",
    "        return cls(word_to_index, word_vectors)\n",
    "    \n",
    "    def get_embedding(self, word):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            word (str)\n",
    "        Returns\n",
    "            an embedding (numpy.ndarray)\n",
    "        \"\"\"\n",
    "        return self.word_vectors[self.word_to_index[word]]\n",
    "\n",
    "    def get_closest_to_vector(self, vector, n=1):\n",
    "        \"\"\"Given a vector, return its n nearest neighbors\n",
    "        \n",
    "        Args:\n",
    "            vector (np.ndarray): should match the size of the vectors \n",
    "                in the Annoy index\n",
    "            n (int): the number of neighbors to return\n",
    "        Returns:\n",
    "            [str, str, ...]: words that are nearest to the given vector. \n",
    "                The words are not ordered by distance \n",
    "        \"\"\"\n",
    "        nn_indices = self.index.get_nns_by_vector(vector, n)\n",
    "        return [self.index_to_word[neighbor] for neighbor in nn_indices]\n",
    "    \n",
    "    def compute_and_print_analogy(self, word1, word2, word3):\n",
    "        \"\"\"Prints the solutions to analogies using word embeddings\n",
    "\n",
    "        Analogies are word1 is to word2 as word3 is to __\n",
    "        This method will print: word1 : word2 :: word3 : word4\n",
    "        \n",
    "        Args:\n",
    "            word1 (str)\n",
    "            word2 (str)\n",
    "            word3 (str)\n",
    "        \"\"\"\n",
    "        vec1 = self.get_embedding(word1)\n",
    "        vec2 = self.get_embedding(word2)\n",
    "        vec3 = self.get_embedding(word3)\n",
    "\n",
    "        # now compute the fourth word's embedding!\n",
    "        spatial_relationship = vec2 - vec1\n",
    "        vec4 = vec3 + spatial_relationship\n",
    "\n",
    "        closest_words = self.get_closest_to_vector(vec4, n=4)\n",
    "        existing_words = set([word1, word2, word3])\n",
    "        closest_words = [word for word in closest_words \n",
    "                             if word not in existing_words] \n",
    "\n",
    "        if len(closest_words) == 0:\n",
    "            print(\"Could not find nearest neighbors for the computed vector!\")\n",
    "            return\n",
    "        \n",
    "        for word4 in closest_words:\n",
    "            print(\"{} : {} :: {} : {}\".format(word1, word2, word3, word4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87a10de-0ac2-4116-864c-7ddd7ec9e197",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55375,
     "status": "ok",
     "timestamp": 1634141274702,
     "user": {
      "displayName": "Colin Bannard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3NcIkMWzsy6zxo7AXqwviYA_hgjuzVq_zKWseQ=s64",
      "userId": "01716265927303848317"
     },
     "user_tz": -60
    },
    "id": "c87a10de-0ac2-4116-864c-7ddd7ec9e197",
    "outputId": "892cf887-29bd-4743-ad57-bc6edae16c30"
   },
   "outputs": [],
   "source": [
    "embeddings = EmbeddingUtil.from_embeddings_file('np.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438ad7ef-c20e-402d-ade5-9eda1bad6983",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 433,
     "status": "ok",
     "timestamp": 1634141320085,
     "user": {
      "displayName": "Colin Bannard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3NcIkMWzsy6zxo7AXqwviYA_hgjuzVq_zKWseQ=s64",
      "userId": "01716265927303848317"
     },
     "user_tz": -60
    },
    "id": "438ad7ef-c20e-402d-ade5-9eda1bad6983",
    "outputId": "1ea8db10-08e1-4812-cd32-689b17ca0c1e"
   },
   "outputs": [],
   "source": [
    "vec=embeddings.get_embedding(\"child\")\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6119bba-e949-48ff-ad40-0b826968177a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 500,
     "status": "ok",
     "timestamp": 1634141322076,
     "user": {
      "displayName": "Colin Bannard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3NcIkMWzsy6zxo7AXqwviYA_hgjuzVq_zKWseQ=s64",
      "userId": "01716265927303848317"
     },
     "user_tz": -60
    },
    "id": "f6119bba-e949-48ff-ad40-0b826968177a",
    "outputId": "5abd6fb7-9ec9-476a-bfd5-9aad284f6b4f"
   },
   "outputs": [],
   "source": [
    "embeddings.get_closest_to_vector(vec, n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fcdc3b-ee92-4181-b901-6fe7ff50cd64",
   "metadata": {
    "id": "d3fcdc3b-ee92-4181-b901-6fe7ff50cd64"
   },
   "source": [
    "# Pretrained Embeddings\n",
    "Vectors are best when learned from very large text collections. However learning such vectors, particular using neural network methods, is very computationally intensive. As a result most people make use of pretrained embeddings such as those found at\n",
    "\n",
    "https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "or\n",
    "\n",
    "https://nlp.stanford.edu/projects/glove/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c47b377-d8ea-4b23-8448-17128d849f0b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 165480,
     "status": "ok",
     "timestamp": 1634144089618,
     "user": {
      "displayName": "Colin Bannard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3NcIkMWzsy6zxo7AXqwviYA_hgjuzVq_zKWseQ=s64",
      "userId": "01716265927303848317"
     },
     "user_tz": -60
    },
    "id": "5c47b377-d8ea-4b23-8448-17128d849f0b",
    "outputId": "c7282b7d-ff0d-46ed-9d97-a7c3ec07c300"
   },
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2rH67GAblByf",
   "metadata": {
    "executionInfo": {
     "elapsed": 28757,
     "status": "ok",
     "timestamp": 1634144143484,
     "user": {
      "displayName": "Colin Bannard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3NcIkMWzsy6zxo7AXqwviYA_hgjuzVq_zKWseQ=s64",
      "userId": "01716265927303848317"
     },
     "user_tz": -60
    },
    "id": "2rH67GAblByf"
   },
   "outputs": [],
   "source": [
    "!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf61f39-face-4645-a849-7f03c840f4bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53492,
     "status": "ok",
     "timestamp": 1634144201208,
     "user": {
      "displayName": "Colin Bannard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3NcIkMWzsy6zxo7AXqwviYA_hgjuzVq_zKWseQ=s64",
      "userId": "01716265927303848317"
     },
     "user_tz": -60
    },
    "id": "baf61f39-face-4645-a849-7f03c840f4bd",
    "outputId": "3b35fa60-26f8-4a0d-fbb5-7abb3056f8dd"
   },
   "outputs": [],
   "source": [
    "embeddings = EmbeddingUtil.from_embeddings_file('glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf9a9d2-eff9-47aa-9e85-aad713a6c19c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 370,
     "status": "ok",
     "timestamp": 1634144595816,
     "user": {
      "displayName": "Colin Bannard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3NcIkMWzsy6zxo7AXqwviYA_hgjuzVq_zKWseQ=s64",
      "userId": "01716265927303848317"
     },
     "user_tz": -60
    },
    "id": "baf9a9d2-eff9-47aa-9e85-aad713a6c19c",
    "outputId": "588edaaa-4cde-46ce-a07d-4f052ad5b43f"
   },
   "outputs": [],
   "source": [
    "vec=embeddings.get_embedding(\"child\")\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382d5835-77ed-4fb4-9452-cae913ebbc61",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 345,
     "status": "ok",
     "timestamp": 1634144597746,
     "user": {
      "displayName": "Colin Bannard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3NcIkMWzsy6zxo7AXqwviYA_hgjuzVq_zKWseQ=s64",
      "userId": "01716265927303848317"
     },
     "user_tz": -60
    },
    "id": "382d5835-77ed-4fb4-9452-cae913ebbc61",
    "outputId": "99c10e92-3745-4fbc-98ed-2086afb62241"
   },
   "outputs": [],
   "source": [
    "embeddings.get_closest_to_vector(vec, n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e0b00f-2595-4402-8492-54e0ee364824",
   "metadata": {},
   "source": [
    "Another semantic property of embeddings is their ability to capture relational meanings. In an important early vector space model of cognition, Rumelhart and Abrahamson (1973) proposed the parallelogram model for solving simple analogy problems of the form a is to b as a* is to what?. In such problems, a system given a problem like apple:tree::grape:?, i.e., apple is to tree as  grape is to , and must fill in the word vine.\n",
    "\n",
    "In the parallelogram model, the vector from the word apple to the word tree (= apple − tree) is added to the vector for grape (grape); the nearest word to that point is returned. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065739ee-7ef2-4471-b33e-6d61c41cf7f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 330,
     "status": "ok",
     "timestamp": 1634144602395,
     "user": {
      "displayName": "Colin Bannard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3NcIkMWzsy6zxo7AXqwviYA_hgjuzVq_zKWseQ=s64",
      "userId": "01716265927303848317"
     },
     "user_tz": -60
    },
    "id": "065739ee-7ef2-4471-b33e-6d61c41cf7f2",
    "outputId": "d9448813-9ae9-415c-927f-6046925be7e3"
   },
   "outputs": [],
   "source": [
    "embeddings.compute_and_print_analogy('fly', 'plane', 'sail')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week_3_Seminar.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
