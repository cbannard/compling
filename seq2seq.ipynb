{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMHcH58NEBbTUavyjfJ9RXw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cbannard/compling/blob/main/seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEQrUh-KmOYE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6af4b6c-7d90-4dea-ef6b-a392bfbad1ee"
      },
      "source": [
        "%reset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import Counter\n",
        "\n",
        "import torchtext\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "from torchtext.legacy.vocab import Vocab\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import platform\n",
        "import os\n",
        "import io\n",
        "import copy\n",
        "global enc, dec, model\n",
        "TFR = 0.5\n",
        "data_path = path = F\"/content/gdrive/My Drive/Modelling_Sentence_Repetition/Code/s2s/\" \n",
        "train_filepaths = [data_path + 'source-all.txt']\n",
        "val_filepaths = train_filepaths\n",
        "test_filepaths = train_filepaths\n"
      ],
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpwfnS1aAie-"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n"
      ],
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZndFAuavmnl"
      },
      "source": [
        "def strip_string(str):\n",
        "    out = ''\n",
        "    for char in str:\n",
        "        if char not in ['“', '”', '[', ']', '<', '>']:\n",
        "            out = out + char\n",
        "    return out"
      ],
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4hx-56luy3w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05a36522-1a1b-4aa9-8f9b-ab8840851b6e"
      },
      "source": [
        "def get_riches_targets():\n",
        "    max_len = 0\n",
        "    print('getting Riches data')\n",
        "    outlist = []\n",
        "    with open( data_path + 'target-lines-Riches-punct.txt', encoding = 'utf-8') as f:\n",
        "        inlist = f.read().split('\\n')\n",
        "    for item in inlist:\n",
        "        index = item.find(',')\n",
        "        if index > 0:\n",
        "            outlist.append(item[index+1:].split())\n",
        "    for item in outlist:\n",
        "        if len(item) > max_len:\n",
        "            max_len = len(item)\n",
        "    print('max length is', max_len)\n",
        "    return outlist\n",
        "\n",
        "test_set = get_riches_targets()\n",
        "\n",
        "src_counts = {}"
      ],
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "getting Riches data\n",
            "max length is 14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rryoJ9arerr9"
      },
      "source": [
        "def build_vocab(filepath, test_set):\n",
        "    counter = Counter()\n",
        "    with open(filepath, encoding=\"utf8\") as infile:\n",
        "        inlist = infile.read().splitlines()\n",
        "        print(len(inlist))\n",
        "        #for string_ in ['<pad>', '<bos>', '<eos>']:\n",
        "        #    for i in range(100):\n",
        "        #        counter.update(string_)\n",
        "        for line in inlist:\n",
        "            line = line.split()\n",
        "            for string_ in line:\n",
        "                ##print(string_)\n",
        "                ## just pass [string] instead of tokenizer\n",
        "                counter.update([strip_string(string_)])\n",
        "        for line in test_set:\n",
        "            for i in range(100):\n",
        "                for string_ in line:\n",
        "                    ##print(string_)\n",
        "                    counter.update(strip_string(string_))\n",
        "    return Vocab(counter, specials=['<unk>', '<bos>', '<eos>', '<pad>'])\n"
      ],
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CX0jXeBd5dK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7793fa4f-7d27-4880-f782-f0bd0ab1c07d"
      },
      "source": [
        "full_vocab = build_vocab(test_filepaths[0], test_set)\n",
        "src_vocab = Vocab(counter=full_vocab.freqs, min_freq=5, specials=['<unk>', '<bos>', '<eos>', '<pad>'])\n",
        "#trg_vocab = build_vocab(train_filepaths[1], trg_tokenizer)\n",
        "trg_vocab = src_vocab\n",
        "del(full_vocab)\n",
        "\n",
        "embedding_length = 50\n",
        "temp_matrix = {}\n",
        "embeddings_matrix = torch.zeros((len(src_vocab), 50))\n",
        "PAD_IDX = trg_vocab.stoi['<pad>']\n",
        "FUNCTION_WORDS = ['a', 'an', 'the',\n",
        "                  'and', 'but', 'with', 'of', 'not', 'as', 'with', 'at', 'more',\n",
        "                  'in', 'on', 'out', 'up', 'of', 'off', 'here', 'there',\n",
        "                 'I', 'you', 'we', 'they', 'me', 'us', 'them', 'mine',\n",
        "                  'yours', 'ours', 'theirs',\n",
        "                  'he', 'she', 'it', 'this', 'that', 'these', 'those', 'him', 'her',\n",
        "                  'is', 'are', 'be', 'was', 'were', \"I'm\", \"you're\", \"he's\", \"she's\",\n",
        "                  \"it's\", \"we're\", \"they're\", \"that's\", \"here's\", \"there's\",\n",
        "                  'have', 'can', 'will', 'had', 'could', 'would', \"can't\", \"won't\",\n",
        "                  'do', \"don't\", 'does'\n",
        "                                 'what', 'where', 'which', 'who', 'how']\n",
        "\n",
        "PRETRAINED = 'False'\n",
        "HID_DIM = 100\n",
        "N_LAYERS = 1\n",
        "ENC_DROPOUT = 0.0\n",
        "DEC_DROPOUT = 0.0\n",
        "INPUT_DIM = len(src_vocab)\n",
        "OUTPUT_DIM = len(trg_vocab)\n",
        "ENC_EMB_DIM = 50\n",
        "DEC_EMB_DIM = 50\n",
        "N_EPOCHS = 2\n",
        "EPOCH_SIZE = 10\n"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1015640\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqw5M3WcGJQW"
      },
      "source": [
        "#train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
        "#                                                  fields = (SRC, TRG))\n",
        "def data_process(filepaths):\n",
        "    MAX_LEN = 15\n",
        "    raw_src_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "    raw_trg_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "    data = []\n",
        "    for (raw_src, raw_trg) in zip(raw_src_iter, raw_trg_iter):\n",
        "        raw_src = raw_src.split()\n",
        "        for i in range(len(raw_src)):\n",
        "            raw_src[i] = strip_string(raw_src[i])\n",
        "        if  len(raw_src) < MAX_LEN:\n",
        "            #raw_src = ['<bos>'] + raw_src + ['<eos>']\n",
        "            #while len(raw_src) < MAX_LEN + 2:\n",
        "            ##    raw_src = ['<pad>'] + raw_src\n",
        "            #    raw_src = raw_src + ['<pad>']\n",
        "            #raw_src = ' '.join(raw_src)\n",
        "            #print(raw_src)\n",
        "            for i in range(len(raw_src)):\n",
        "                ##raw_src[i] = strip_string(raw_src[i])\n",
        "                if src_vocab.stoi[raw_src[i]] == 0:\n",
        "                    #print('unk', raw_src[i])\n",
        "                    int = random.randint(4, len(src_vocab) -1)\n",
        "                    #print(int)\n",
        "                    #print('random', src_vocab.itos[int])\n",
        "                    raw_src[i] = src_vocab.itos[int]\n",
        "            raw_trg = copy.deepcopy(raw_src)\n",
        "            #raw_src = ['<bos>'] + raw_src + ['<eos>']\n",
        "            raw_trg = ['<bos>'] + raw_trg + ['<eos>']\n",
        "            while len(raw_src) < MAX_LEN:\n",
        "                #raw_src = raw_src + ['<pad>']\n",
        "                raw_src = ['<pad>'] + raw_src\n",
        "                raw_trg = raw_trg +  ['<pad>']\n",
        "            #src_tensor_ = torch.tensor([src_vocab[token] for token in raw_src],\n",
        "            #                            #src_tokenizer(raw_src)],\n",
        "            #    dtype=torch.long)\n",
        "            src_tensor_ = torch.tensor([src_vocab[token] for token in raw_src], dtype=torch.long)\n",
        "            #print(len(src_tensor_))\n",
        "            if src_tensor_[-1] == src_vocab.stoi['\\n']:\n",
        "                src_tensor_ = src_tensor_[0:-1]\n",
        "            trg_tensor_ = torch.tensor([src_vocab[token] for token in raw_trg], dtype=torch.long)\n",
        "        #print(len(src_tensor_))\n",
        "            if trg_tensor_[-1] == src_vocab.stoi['\\n']:\n",
        "                trg_tensor_ = src_tensor_[0:-1]\n",
        "            #trg_tensor_ = torch.tensor([src_vocab[token] for token in src_tokenizer(raw_trg)],\n",
        "            #                           dtype=torch.long)\n",
        "            #if trg_tensor_[-1] == src_vocab.stoi['\\n']:\n",
        "            #    trg_tensor_ = trg_tensor_[0:-1]\n",
        "            data.append((src_tensor_, trg_tensor_))\n",
        "    return data\n",
        "    \n",
        "train_data = data_process(train_filepaths)\n",
        "val_data = data_process(val_filepaths)\n",
        "test_data = data_process(test_filepaths)"
      ],
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsmMhjY37xQY"
      },
      "source": [
        "def load_embs():\n",
        "    global enc, dec, model\n",
        "    print_emb('wants')\n",
        "    for file in ['enc-embs-1000.out', 'dec-embs-1000.out']:\n",
        "        with open(data_path + file, 'r', encoding = 'utf-8') as enc_file:\n",
        "            inlist = enc_file.read().splitlines()\n",
        "            print(len(inlist))\n",
        "            for line in inlist:\n",
        "                line = line.split(',')\n",
        "                word, emb = line[0], ' '.join(line[1:])\n",
        "                #print(word,emb)\n",
        "                if word not in FUNCTION_WORDS:\n",
        "                    emb = torch.tensor(np.fromstring(emb, sep = ' '))\n",
        "                    if 'enc-embs' in file:\n",
        "                        int = src_vocab.stoi[word]\n",
        "                        #print(word, len(word), int)\n",
        "                        if word == 'wants':\n",
        "                            print('wants', enc.embedding.weight.data[int])\n",
        "                        enc.embedding.weight.data[int] = emb\n",
        "                        if word == 'wants':\n",
        "                            print('enc', enc.embedding.weight.data[int])\n",
        "                    elif 'dec-embs' in file:\n",
        "                        ##print('dec')\n",
        "                        int = trg_vocab.stoi[word]\n",
        "                        dec.embedding.weight.data[int] = emb\n",
        "    print_emb('wants')\n",
        "    print_emb('is')\n",
        "\n",
        "def print_emb(word):\n",
        "    global enc\n",
        "    lookup = torch.tensor([src_vocab.stoi[word]], dtype=torch.long)\n",
        "    emb = enc.embedding.weight.data[lookup]\n",
        "    print(emb)"
      ],
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LQfrNOyYJDy"
      },
      "source": [
        "def generate_batch(data_batch):\n",
        "    #print('len in', len(data_batch))\n",
        "    #print(data_batch)\n",
        "    src_batch, trg_batch = [], []\n",
        "    for (src_item, trg_item) in data_batch:\n",
        "        ##### I think we only want to add beg and end markers to the target, not to the source...\n",
        "        #src_batch.append(src_item)\n",
        "        #trg_batch.append(trg_item)\n",
        "        src_batch.append(torch.cat([src_item], dim=0))\n",
        "        trg_batch.append(torch.cat([trg_item], dim=0))\n",
        "        #src_batch.append(torch.cat([torch.tensor([BOS_IDX]), src_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "        #trg_batch.append(torch.cat([torch.tensor([BOS_IDX]), trg_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX)\n",
        "    #print('len out', len(src_batch))\n",
        "    #print(src_batch)\n",
        "    return src_batch, trg_batch\n"
      ],
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BacwRm7GztH"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kyBhDeqG6ZQ"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout, weights_matrix):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        if PRETRAINED == 'False':\n",
        "            self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx= PAD_IDX)\n",
        "        elif PRETRAINED == 'True':\n",
        "            print('encoder using trained embeddings')\n",
        "            self.embedding = nn.Embedding.from_pretrained(weights_matrix, freeze = 'False',padding_idx = PAD_IDX)\n",
        "\n",
        "        ##self.embedding = create_emb_layer(weights_matrix)\n",
        "        lookup = torch.tensor([src_vocab.stoi['baby']], dtype=torch.long)\n",
        "        emb = self.embedding(lookup)\n",
        "        #print(emb)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "\n",
        "        #src = [src len, batch size]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        #print('done')\n",
        "        #print(embedded[209])\n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "\n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        #outputs are always from the top hidden layer\n",
        "        ##print('cell', cell[0][0])\n",
        "        ## add random noise (mean 0, var 1) to the cell state\n",
        "        rndm = torch.randn(cell.size()).to(device)\n",
        "        #cell = cell + (0.5 * rndm)\n",
        "        #hidden = hidden + (0.5 * rndm)\n",
        "        #print('rand', rndm[0][0])\n",
        "        #print('new', cell[0][0])\n",
        "        #for name in Encoder.named_parameters(self):\n",
        "        #    if 'weight' in name[0]:\n",
        "        #        print('weights', name)\n",
        "        return hidden, cell\n"
      ],
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtqBBcHEG_Q7"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout, weights_matrix):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        if PRETRAINED == 'False':\n",
        "            self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx = PAD_IDX)\n",
        "        elif PRETRAINED == 'True':\n",
        "            print('decoder using trained embeddings')\n",
        "            self.embedding = nn.Embedding.from_pretrained(weights_matrix, freeze = 'False',\n",
        "                                                         padding_idx= PAD_IDX)\n",
        "        ##self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        ##self.embedding = nn.Embedding.from_pretrained(weights_matrix, freeze = 'False')\n",
        "        lookup = torch.tensor([src_vocab.stoi['baby']], dtype=torch.long)\n",
        "        emb = self.embedding(lookup)\n",
        "        ##print(emb)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "\n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        #n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #context = [n layers, batch size, hid dim]\n",
        "\n",
        "        input = input.unsqueeze(0)\n",
        "\n",
        "        #input = [1, batch size]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "        #embedded = [1, batch size, emb dim]\n",
        "\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        #print('out', output)\n",
        "        #print('hid', hidden)\n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        rndm = torch.randn(cell.size()).to(device)\n",
        "        #cell = cell + (0.25 * rndm)\n",
        "        #hidden = hidden + (0.25 * rndm)\n",
        "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #cell = [n layers, batch size, hid dim]\n",
        "\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "\n",
        "        #prediction = [batch size, output dim]\n",
        "\n",
        "        return prediction, hidden, cell\n"
      ],
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlfJE5HsI-az"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        #print(\"TFR =\", teacher_forcing_ratio)\n",
        "        #print('fw src', src)\n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "\n",
        "            #insert input token embedding, previous hidden and previous cell states\n",
        "            #receive output tensor (predictions) and new hidden and cell states\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "\n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "\n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            #print('tf', teacher_force)\n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1)\n",
        "\n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjeOtxOSJae_"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "def train(model: nn.Module,\n",
        "          iterator: torch.utils.data.DataLoader,\n",
        "          optimizer: optim.Optimizer,\n",
        "          criterion: nn.Module,\n",
        "          clip: float):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for _, (src, trg) in enumerate(iterator):\n",
        "        ##print('src', src)\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg)\n",
        "\n",
        "        output = output[1:].view(-1, output.shape[-1])\n",
        "        trg = trg[1:].view(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sqk12W2Jfhr"
      },
      "source": [
        "def evaluate(model: nn.Module,\n",
        "             iterator: torch.utils.data.DataLoader,\n",
        "             criterion: nn.Module):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for _, (src, trg) in enumerate(iterator):\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            output = output[1:].view(-1, output.shape[-1])\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yopqp1bdJqmL"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n85PqMu2bB0I"
      },
      "source": [
        "def test_manc():\n",
        "    test_list = []\n",
        "    count = 0\n",
        "    test_list = [\"what do you want the thick pens\"]\n",
        "    #, \"she was quite happily pulling it\", \"<unk> these <unk> <unk> are these your water wings\", \"do you think #the policeman'll be after him\", \"that's <unk> that's food for the cat\", \"is it a big hat that\", \"are you #building it up or breaking it up\", \"there were two on there earlier , weren't there\", \"well I can't throw it #now , can I\", \"and what does a cow say\"]\n",
        "    #while len(test_list) < 10:\n",
        "    #    line = tensor_to_line(random.choice(train_data))\n",
        "    #    print(line)\n",
        "    #    if (len(line) > 5\n",
        "    #        and len(line) <10):\n",
        "    #        test_list.append(' '.join(line))\n",
        "    for line in test_list:\n",
        "        print(line)\n",
        "        output = translate_sentence(line, model, 'cuda', 50)\n",
        "        print('input', line)\n",
        "        print('output', output)\n",
        "        count += len(output)\n",
        "    print('word count', count)"
      ],
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPT9LEs1f9hd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffe66107-366c-48f9-869c-b3c906a3ad0a"
      },
      "source": [
        "out_dir = data_path + \"saved-models/\"\n",
        "\n",
        "def count_parameters(model: nn.Module):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def run_model(name_string):\n",
        "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "    CLIP = 1\n",
        "    print('out_dir', out_dir)\n",
        "    print('hid_dim', HID_DIM)\n",
        "    print('TFR', TFR)\n",
        "    print('epochs', N_EPOCHS)\n",
        "    best_valid_loss = float('inf')\n",
        "    MAX_LEN = 20\n",
        "    max_target = 200\n",
        "    name_string = name_string + str(HID_DIM) + '-50-ep-'\n",
        "    print('name_string', name_string)\n",
        "    for epoch in range(1, N_EPOCHS +1):\n",
        "        part_tr, part_va = [], []\n",
        "        while len(part_tr) < 8000:\n",
        "            part_tr.append(random.choice(train_data))\n",
        "        while len(part_va) < 2000:\n",
        "            part_va.append(random.choice(train_data))\n",
        "        print('tr', part_tr[0])\n",
        "        print('va', part_va[0])\n",
        "        #for i in range(len(part_tr)):\n",
        "        #    if len(part_tr[i][1]) > max_target:\n",
        "        #        part_tr[i] = [part_tr[i][0], truncate_random(part_tr[i][1], max=max_target)]\n",
        "        #        ##part_tr[i][1] = truncate_random(part_tr[i][1], max_target)\n",
        "        #        #print('done')\n",
        "        #        #print('in, out', part_tr[i][0], part_tr[i][1])\n",
        "        #train_iter = DataLoader(part_tr, batch_size=args.batch_size, shuffle=True, num_workers=4, **kwargs)\n",
        "        train_iter = DataLoader(part_tr, batch_size=BATCH_SIZE,\n",
        "                                shuffle=True, collate_fn=generate_batch)\n",
        "        valid_iter = DataLoader(part_va, batch_size=BATCH_SIZE,\n",
        "                                shuffle=True, collate_fn=generate_batch)\n",
        "        #test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
        "        #                       shuffle=True, collate_fn=generate_batch)\n",
        "\n",
        "\n",
        "        start_time = time.time()\n",
        "        train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
        "        valid_loss = evaluate(model, valid_iter, criterion)\n",
        "        del(part_tr)\n",
        "        del(part_va)\n",
        "        del(train_iter)\n",
        "        del(valid_iter)\n",
        "        end_time = time.time()\n",
        "\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        ##print('epoch', epoch+1, epoch_mins, epoch_secs)\n",
        "        print(f'Epoch: {epoch:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "        torch.cuda.empty_cache()\n",
        "        if (epoch > 0\n",
        "                and epoch % 10 == 0):\n",
        "            test_riches(name_string + str(epoch))\n",
        "            test_manc()\n",
        "            torch.save(model.state_dict(), out_dir + name_string + str(epoch) + '.pth')\n",
        "        #if (epoch > 0\n",
        "        #            and epoch % 50 == 0):\n",
        "        #    add_noise()\n",
        "\n",
        "N_EPOCHS = 2\n",
        "\n",
        "PRETRAINED = 'False'\n",
        "for HID_DIM in [500]:\n",
        "    #[250, 500]:\n",
        "        for i in range(1,6):\n",
        "            BATCH_SIZE = 10\n",
        "            print('TFR=', TFR)\n",
        "            print('run is', str(i))\n",
        "            name_string = 'pt-tf05-500-run-' + str(i) +'-wemb-06-07-'\n",
        "            enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT, embeddings_matrix)\n",
        "            ##enc.embedding.weight.data = embeddings_matrix\n",
        "            dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT, embeddings_matrix)\n",
        "            model = Seq2Seq(enc, dec, device).to(device)\n",
        "            model.apply(init_weights)\n",
        "            optimizer = optim.Adam(model.parameters())\n",
        "            load_embs()\n",
        "            print(model.eval())\n",
        "            run_model(name_string)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TFR= 0.5\n",
            "run is 1\n",
            "tensor([[-0.0317, -0.0068, -0.0656,  0.0148, -0.0166, -0.0284,  0.0234, -0.0314,\n",
            "          0.0159,  0.0717,  0.0193, -0.0390, -0.0331, -0.0105,  0.0758,  0.0126,\n",
            "         -0.0195, -0.0372,  0.0225,  0.0580,  0.0777, -0.0047,  0.0727, -0.0266,\n",
            "          0.0326, -0.0063, -0.0696, -0.0692,  0.0568,  0.0499, -0.0170, -0.0157,\n",
            "          0.0134, -0.0720,  0.0370, -0.0537, -0.0584, -0.0115, -0.0559,  0.0037,\n",
            "          0.0054,  0.0036, -0.0099, -0.0752, -0.0618,  0.0649, -0.0317,  0.0122,\n",
            "          0.0021,  0.0564]])\n",
            "10359\n",
            "wants tensor([-0.0317, -0.0068, -0.0656,  0.0148, -0.0166, -0.0284,  0.0234, -0.0314,\n",
            "         0.0159,  0.0717,  0.0193, -0.0390, -0.0331, -0.0105,  0.0758,  0.0126,\n",
            "        -0.0195, -0.0372,  0.0225,  0.0580,  0.0777, -0.0047,  0.0727, -0.0266,\n",
            "         0.0326, -0.0063, -0.0696, -0.0692,  0.0568,  0.0499, -0.0170, -0.0157,\n",
            "         0.0134, -0.0720,  0.0370, -0.0537, -0.0584, -0.0115, -0.0559,  0.0037,\n",
            "         0.0054,  0.0036, -0.0099, -0.0752, -0.0618,  0.0649, -0.0317,  0.0122,\n",
            "         0.0021,  0.0564])\n",
            "enc tensor([ 0.1927,  0.1655,  0.0019, -0.1502,  0.1600,  0.0017, -0.2758, -0.3668,\n",
            "         0.4246,  0.3457,  0.3559, -0.3872,  0.0376,  0.3435, -0.5560,  0.3198,\n",
            "        -0.0520, -0.5638, -0.0738,  0.2121, -0.1119,  0.0682, -0.1407,  0.2428,\n",
            "        -0.0775, -0.4191,  0.3381,  0.1537,  0.2311,  0.3229,  0.0125,  0.2666,\n",
            "         0.1895, -0.1156, -0.1083, -0.2899,  0.2875,  0.1523, -0.1518, -0.1016,\n",
            "        -0.3404,  0.2787,  0.4194, -0.2447,  0.3855, -0.3031, -0.0535, -0.2409,\n",
            "        -0.4400, -0.2921])\n",
            "10359\n",
            "tensor([[ 0.1927,  0.1655,  0.0019, -0.1502,  0.1600,  0.0017, -0.2758, -0.3668,\n",
            "          0.4246,  0.3457,  0.3559, -0.3872,  0.0376,  0.3435, -0.5560,  0.3198,\n",
            "         -0.0520, -0.5638, -0.0738,  0.2121, -0.1119,  0.0682, -0.1407,  0.2428,\n",
            "         -0.0775, -0.4191,  0.3381,  0.1537,  0.2311,  0.3229,  0.0125,  0.2666,\n",
            "          0.1895, -0.1156, -0.1083, -0.2899,  0.2875,  0.1523, -0.1518, -0.1016,\n",
            "         -0.3404,  0.2787,  0.4194, -0.2447,  0.3855, -0.3031, -0.0535, -0.2409,\n",
            "         -0.4400, -0.2921]])\n",
            "tensor([[ 0.0547,  0.0359, -0.0016,  0.0780, -0.0458, -0.0008,  0.0476,  0.0785,\n",
            "         -0.0725,  0.0308,  0.0071,  0.0483, -0.0051,  0.0228,  0.0183,  0.0222,\n",
            "          0.0634, -0.0009,  0.0057, -0.0025, -0.0240,  0.0467, -0.0422, -0.0134,\n",
            "         -0.0005,  0.0370, -0.0398, -0.0327, -0.0124,  0.0133,  0.0352, -0.0662,\n",
            "         -0.0407,  0.0070, -0.0021,  0.0367,  0.0262,  0.0090, -0.0049,  0.0252,\n",
            "         -0.0327, -0.0011, -0.0646,  0.0788, -0.0717, -0.0330,  0.0571,  0.0218,\n",
            "         -0.0193,  0.0452]])\n",
            "Seq2Seq(\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Embedding(10359, 50, padding_idx=3)\n",
            "    (rnn): LSTM(50, 500)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (embedding): Embedding(10359, 50, padding_idx=3)\n",
            "    (rnn): LSTM(50, 500)\n",
            "    (fc_out): Linear(in_features=500, out_features=10359, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            ")\n",
            "The model has 8,433,759 trainable parameters\n",
            "out_dir /content/gdrive/My Drive/Modelling_Sentence_Repetition/Code/s2s/saved-models/\n",
            "hid_dim 500\n",
            "TFR 0.5\n",
            "epochs 2\n",
            "name_string pt-tf05-500-run-1-wemb-06-07-500-50-ep-\n",
            "tr (tensor([ 3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3, 89, 18, 25, 61]), tensor([ 1, 89, 18, 25, 61,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3]))\n",
            "va (tensor([   3,    3,    3,    3,    3,    3,    3,  147,   63,  197,  139,  533,\n",
            "         168,    5, 5648]), tensor([   1,  147,   63,  197,  139,  533,  168,    5, 5648,    2,    3,    3,\n",
            "           3,    3,    3,    3,    3]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eAVclZEagT0",
        "outputId": "ceb75174-5f43-4a12-f47b-5c84e7b74f5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "def translate_sentence(sentence = 'and what does a cow say', model = model, device = 'cuda', max_len = 50):\n",
        " \n",
        "    sentence = \"there were two on there earlier , weren't there\"\n",
        "    print(\"INPUT:\", sentence)\n",
        "    #if isinstance(sentence, str):\n",
        "    #    sentence = '<bos> ' + sentence + ' <eos>'\n",
        "    #    sentence = sentence.split()\n",
        "    #else:\n",
        "    #    sentence = ['<bos>' + sentence + '<eos']\n",
        "    #    sentence = sentence.split()\n",
        "    sentence =sentence.split()\n",
        "  \n",
        "    tokens = [token.lower() for token in sentence]\n",
        "    #print(tokens)\n",
        "    src_indexes = [src_vocab.stoi[token] for token in tokens]\n",
        "    ##print(src_indexes)\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "    ##print(src_tensor)\n",
        "    src_len = torch.LongTensor([len(src_indexes)])\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(src_tensor)\n",
        "    print('hidden', hidden)\n",
        "    ##mask = model.create_mask(src_tensor)\n",
        "    trg_indexes = [trg_vocab.stoi['<bos>']]\n",
        "    output_indexes = []\n",
        "\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(trg_tensor, hidden.to(device), cell.to(device))\n",
        "        \n",
        "        res, ind = output.topk(3)\n",
        "        print(res, len(res), ind)\n",
        "\n",
        "        for k in range(len(res[0])):\n",
        "            print('0', res[0][k])\n",
        "            print('1', k, res[0][k].item())\n",
        "            print('2', k, ind[0][k].item())\n",
        "            print('3', k, trg_vocab.itos[ind[0][k].item()])\n",
        "        print('4', (res[0][0])/(res[0][1]) + res[0][2])\n",
        "        print('5', res[0][0].item()/(res[0][1].item()/res[0][2].item()))\n",
        "\n",
        "        if (res[0][0] > 0.60 * (res[0][1] + res[0][2])\n",
        "            or trg_vocab.itos[output.argmax(1).item()] == '<eos>'):\n",
        "            pred_token = output.argmax(1).item()\n",
        "        else:\n",
        "            pred_token = trg_vocab.stoi['<pad>']\n",
        "        print('pred', pred_token)\n",
        "\n",
        "        trg_indexes.append(output.argmax(1).item())\n",
        "\n",
        "        output_indexes.append(pred_token)\n",
        "     \n",
        "        if pred_token == trg_vocab.stoi['<eos>']:\n",
        "            break\n",
        "\n",
        "    output_tokens = [trg_vocab.itos[i] for i in output_indexes]\n",
        "    return(output_tokens)\n",
        "\n",
        "\n"
      ],
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-264-1fe1609ae5f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'and what does a cow say'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cuda'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"there were two on there earlier , weren't there\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"INPUT:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#if isinstance(sentence, str):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    }
  ]
}