{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPOGlztg9EZKzbo4K5L++Wh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cbannard/compling/blob/main/seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEQrUh-KmOYE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e05f1af2-1db7-4718-9392-05a82fea9cbb"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "\n",
        "import torchtext\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import platform\n",
        "import os\n",
        "global enc, dec, model\n",
        "TFR = 0.5\n",
        "data_path = path = F\"/content/gdrive/My Drive/Modelling_Sentence_Repetition/Code/s2s/\" \n",
        "train_filepaths = [data_path + 'source-all.txt']\n",
        "val_filepaths = train_filepaths\n",
        "test_filepaths = train_filepaths\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpwfnS1aAie-"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tefKqGVLf1cv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZO_7kITApQv"
      },
      "source": [
        "SRC = Field(tokenize = word_tokenize, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)\n",
        "\n",
        "TRG = Field(word_tokenize, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqw5M3WcGJQW"
      },
      "source": [
        "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
        "                                                    fields = (SRC, TRG))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTaxjgvbGxlt"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4hx-56luy3w",
        "outputId": "d4f0b12d-4e23-4325-eb56-24252c7a438d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def get_riches_targets():\n",
        "    max_len = 0\n",
        "    print('getting Riches data')\n",
        "    outlist = []\n",
        "    with open( data_path + 'target-lines-Riches-punct.txt', encoding = 'utf-8') as f:\n",
        "        inlist = f.read().split('\\n')\n",
        "    for item in inlist:\n",
        "        index = item.find(',')\n",
        "        if index > 0:\n",
        "            outlist.append(item[index+1:].split())\n",
        "    for item in outlist:\n",
        "        if len(item) > max_len:\n",
        "            max_len = len(item)\n",
        "    print('max length is', max_len)\n",
        "    return outlist\n",
        "\n",
        "test_set = get_riches_targets()\n",
        "\n",
        "src_counts = {}"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "getting Riches data\n",
            "max length is 14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rryoJ9arerr9"
      },
      "source": [
        "def build_vocab(filepath, test_set):\n",
        "    counter = Counter()\n",
        "    with open(filepath, encoding=\"utf8\") as infile:\n",
        "        inlist = infile.read().splitlines()\n",
        "        print(len(inlist))\n",
        "        #for string_ in ['<pad>', '<bos>', '<eos>']:\n",
        "        #    for i in range(100):\n",
        "        #        counter.update(string_)\n",
        "        for line in inlist:\n",
        "            line = line.split()\n",
        "            for string_ in line:\n",
        "                ##print(string_)\n",
        "                ## just pass [string] instead of tokenizer\n",
        "                counter.update([strip_string(string_)])\n",
        "        for line in test_set:\n",
        "            for i in range(100):\n",
        "                for string_ in line:\n",
        "                    ##print(string_)\n",
        "                    counter.update(strip_string(string_))\n",
        "    return Vocab(counter, specials=['<unk>', '<bos>', '<eos>', '<pad>'])\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CX0jXeBd5dK",
        "outputId": "e7e6be83-9e9e-4f81-8117-91d29230faf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "source": [
        "full_vocab = build_vocab(test_filepaths[0], test_set)\n",
        "src_vocab = torchtext.vocab.Vocab(counter=full_vocab.freqs, min_freq=5, specials=['<unk>', '<bos>', '<eos>', '<pad>'])\n",
        "#trg_vocab = build_vocab(train_filepaths[1], trg_tokenizer)\n",
        "trg_vocab = src_vocab\n",
        "del(full_vocab)\n",
        "\n",
        "embedding_length = 50\n",
        "temp_matrix = {}\n",
        "embeddings_matrix = torch.zeros((len(src_vocab), 50))\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-592f56217f83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfull_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_filepaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msrc_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<unk>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'<bos>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'<eos>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#trg_vocab = build_vocab(train_filepaths[1], trg_tokenizer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrg_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc_vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-e8b5b77c2186>\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(filepath, test_set)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0minlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Counter' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BacwRm7GztH"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4vcMn2IMKVC"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE, \n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kyBhDeqG6ZQ"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        \n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        \n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #outputs are always from the top hidden layer\n",
        "        \n",
        "        return hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtqBBcHEG_Q7"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, cell):\n",
        "        \n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #context = [n layers, batch size, hid dim]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "\n",
        "         #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "                \n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        \n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #cell = [n layers, batch size, hid dim]\n",
        "        \n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlfJE5HsI-az"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        \n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "        \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden and previous cell states\n",
        "            #receive output tensor (predictions) and new hidden and cell states\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cumeSbtoJMn1"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-omtzS2JNQR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "965aba3f-4e72-4c8b-cabb-1bb2bf0a9b53"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(7845, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(7704, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (fc_out): Linear(in_features=512, out_features=7704, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMNjhdsOJXiy"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1ZXYDhnJZzP"
      },
      "source": [
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjeOtxOSJae_"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg)\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sqk12W2Jfhr"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            print(output)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yopqp1bdJqmL"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iftad7XbJrP9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0b21bee-312b-4114-d9c4-c0eed4312616"
      },
      "source": [
        "N_EPOCHS = 3\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 27m 18s\n",
            "\tTrain Loss: 3.297 | Train PPL:  27.020\n",
            "\t Val. Loss: 3.952 |  Val. PPL:  52.044\n",
            "Epoch: 02 | Time: 27m 3s\n",
            "\tTrain Loss: 3.170 | Train PPL:  23.815\n",
            "\t Val. Loss: 3.906 |  Val. PPL:  49.704\n",
            "Epoch: 03 | Time: 27m 12s\n",
            "\tTrain Loss: 3.027 | Train PPL:  20.643\n",
            "\t Val. Loss: 3.852 |  Val. PPL:  47.075\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDrsCNM0Snog",
        "outputId": "af1fe3fe-6926-41df-deeb-1ba14b27e51c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLpR8V3zTGKR"
      },
      "source": [
        "model.load_state_dict(torch.load('tut1-model.pt'))\n",
        "path = F\"/content/gdrive/My Drive/tut1-model.pt\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOlV-Q5dJztc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b451e60e-d2f6-459a-83db-84add172d16c"
      },
      "source": [
        "model.load_state_dict(torch.load('tut1-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 5.5188, -7.3904, -6.5965,  ..., -7.2304, -7.2678, -6.5158],\n",
            "        [ 5.4187, -9.1561, -7.5463,  ..., -6.0830, -6.7973, -5.3052],\n",
            "        [ 5.8970, -5.9536, -5.3926,  ..., -6.4508, -4.7705, -4.9962],\n",
            "        ...,\n",
            "        [ 5.8009, -7.6231, -5.7352,  ..., -5.5331, -6.5965, -7.5182],\n",
            "        [ 6.3387, -7.9489, -6.4208,  ..., -4.3347, -6.3744, -7.8730],\n",
            "        [ 5.6698, -7.6268, -6.5530,  ..., -3.9665, -6.0154, -8.7844]])\n",
            "tensor([[ 4.0191, -5.2919, -5.8278,  ..., -5.0277, -5.4157, -4.4312],\n",
            "        [ 4.2264, -8.3667, -7.5391,  ..., -8.3577, -7.7305, -6.1178],\n",
            "        [ 4.6301, -6.9763, -6.3382,  ..., -7.4339, -9.2199, -5.2434],\n",
            "        ...,\n",
            "        [ 6.0032, -8.5511, -7.4861,  ..., -5.8049, -6.6813, -5.7271],\n",
            "        [ 6.4811, -7.9516, -7.2906,  ..., -3.5158, -7.8159, -6.0269],\n",
            "        [ 5.5195, -7.1183, -6.4749,  ..., -3.6279, -3.7435, -7.5254]])\n",
            "tensor([[ 4.9541, -7.5637, -6.3644,  ..., -6.3101, -5.9431, -6.5570],\n",
            "        [ 3.6040, -7.2824, -7.9983,  ..., -6.8790, -6.8412, -5.7228],\n",
            "        [ 3.8169, -5.5160, -6.3860,  ..., -5.4438, -4.2568, -4.8005],\n",
            "        ...,\n",
            "        [ 7.0523, -9.3425, -7.8246,  ..., -2.9418, -6.0582, -4.3268],\n",
            "        [ 6.8716, -9.0659, -7.7854,  ...,  0.1320, -3.0601, -6.3847],\n",
            "        [ 7.2237, -7.6890, -5.9473,  ..., -4.1908, -8.2546, -5.8092]])\n",
            "tensor([[ 5.1051, -7.2938, -6.3933,  ..., -7.6528, -7.9204, -6.6670],\n",
            "        [ 4.5599, -7.4857, -6.3887,  ..., -6.8594, -6.2958, -5.8023],\n",
            "        [ 4.6133, -7.9587, -6.8600,  ..., -8.4796, -7.9089, -5.5260],\n",
            "        ...,\n",
            "        [ 6.2987, -6.8542, -6.1563,  ..., -4.1107, -5.7796, -9.4463],\n",
            "        [ 5.9095, -8.1677, -7.5952,  ..., -2.5369, -5.8357, -5.1233],\n",
            "        [ 5.8762, -7.1683, -6.3690,  ..., -3.8591, -7.8161, -7.5175]])\n",
            "tensor([[ 4.0636, -8.4637, -7.4564,  ..., -8.0659, -7.3694, -6.5137],\n",
            "        [ 4.4225, -6.8586, -5.7681,  ..., -6.4042, -6.1807, -5.0019],\n",
            "        [ 4.0326, -5.1474, -4.9530,  ..., -4.4405, -4.5268, -3.4727],\n",
            "        ...,\n",
            "        [ 6.1956, -7.9713, -7.3975,  ..., -3.5797, -8.2648, -4.6284],\n",
            "        [ 6.0103, -7.5520, -6.6029,  ..., -0.7089, -3.9905, -5.8914],\n",
            "        [ 6.8767, -9.2894, -7.1573,  ..., -5.6021, -6.8928, -7.5880]])\n",
            "tensor([[ 5.7379, -6.0032, -4.9610,  ..., -6.1169, -6.5529, -3.6303],\n",
            "        [ 5.1873, -6.2257, -5.1588,  ..., -6.2993, -5.5652, -3.4068],\n",
            "        [ 4.5045, -6.9700, -5.6093,  ..., -6.9090, -5.5698, -3.9392],\n",
            "        ...,\n",
            "        [ 5.8661, -8.8091, -7.0519,  ..., -4.0393, -6.7393, -3.6927],\n",
            "        [ 5.9090, -8.0460, -6.8996,  ..., -3.5376, -6.8182, -7.0000],\n",
            "        [ 5.1176, -7.9037, -6.9361,  ..., -5.5539, -5.0024, -5.2062]])\n",
            "tensor([[ 5.3522, -6.6183, -5.9904,  ..., -6.8393, -5.2003, -5.1687],\n",
            "        [ 5.7469, -6.1850, -5.4597,  ..., -5.4655, -6.7030, -3.5900],\n",
            "        [ 4.5189, -6.8783, -6.2829,  ..., -6.4825, -9.4307, -2.8198],\n",
            "        ...,\n",
            "        [ 5.2847, -8.3636, -7.4839,  ..., -3.2939, -5.4542, -5.9000],\n",
            "        [ 5.4156, -7.3027, -5.9829,  ..., -4.5984, -6.8312, -6.4398],\n",
            "        [ 5.7918, -7.8511, -6.4219,  ..., -4.1575, -6.5244, -6.6003]])\n",
            "tensor([[ 4.9804, -7.4917, -6.5634,  ..., -8.1453, -7.7005, -6.6131],\n",
            "        [ 5.0811, -7.4036, -6.3841,  ..., -8.3398, -8.0809, -6.5302],\n",
            "        [ 4.7319, -7.5819, -6.6389,  ..., -6.3652, -6.5201, -3.0499],\n",
            "        ...,\n",
            "        [ 5.4028, -7.6582, -7.2526,  ..., -3.6129, -5.3162, -6.7542],\n",
            "        [ 4.8383, -7.9668, -6.3518,  ..., -4.9865, -6.0849, -6.5189],\n",
            "        [ 5.3941, -8.0433, -7.2537,  ..., -4.0890, -5.8914, -6.4809]])\n",
            "| Test Loss: 3.879 | Test PPL:  48.371 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eAVclZEagT0"
      },
      "source": [
        "def translate_sentence(sentence = 'and what does a cow say', model = model, device = 'cuda', max_len = 50):\n",
        " \n",
        "    sentence = \"there were two on there earlier , weren't there\"\n",
        "    print(\"INPUT:\", sentence)\n",
        "    #if isinstance(sentence, str):\n",
        "    #    sentence = '<bos> ' + sentence + ' <eos>'\n",
        "    #    sentence = sentence.split()\n",
        "    #else:\n",
        "    #    sentence = ['<bos>' + sentence + '<eos']\n",
        "    #    sentence = sentence.split()\n",
        "    sentence =sentence.split()\n",
        "  \n",
        "    tokens = [token.lower() for token in sentence]\n",
        "    #print(tokens)\n",
        "    src_indexes = [src_vocab.stoi[token] for token in tokens]\n",
        "    ##print(src_indexes)\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "    ##print(src_tensor)\n",
        "    src_len = torch.LongTensor([len(src_indexes)])\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(src_tensor)\n",
        "    print('hidden', hidden)\n",
        "    ##mask = model.create_mask(src_tensor)\n",
        "    trg_indexes = [trg_vocab.stoi['<bos>']]\n",
        "    output_indexes = []\n",
        "\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(trg_tensor, hidden.to(device), cell.to(device))\n",
        "        \n",
        "        res, ind = output.topk(3)\n",
        "        print(res, len(res), ind)\n",
        "\n",
        "        for k in range(len(res[0])):\n",
        "            print('0', res[0][k])\n",
        "            print('1', k, res[0][k].item())\n",
        "            print('2', k, ind[0][k].item())\n",
        "            print('3', k, trg_vocab.itos[ind[0][k].item()])\n",
        "        print('4', (res[0][0])/(res[0][1]) + res[0][2])\n",
        "        print('5', res[0][0].item()/(res[0][1].item()/res[0][2].item()))\n",
        "\n",
        "        if (res[0][0] > 0.60 * (res[0][1] + res[0][2])\n",
        "            or trg_vocab.itos[output.argmax(1).item()] == '<eos>'):\n",
        "            pred_token = output.argmax(1).item()\n",
        "        else:\n",
        "            pred_token = trg_vocab.stoi['<pad>']\n",
        "        print('pred', pred_token)\n",
        "\n",
        "        trg_indexes.append(output.argmax(1).item())\n",
        "\n",
        "        output_indexes.append(pred_token)\n",
        "     \n",
        "        if pred_token == trg_vocab.stoi['<eos>']:\n",
        "            break\n",
        "\n",
        "    output_tokens = [trg_vocab.itos[i] for i in output_indexes]\n",
        "    return(output_tokens)\n",
        "\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n85PqMu2bB0I"
      },
      "source": [
        "def test_manc():\n",
        "    test_list = []\n",
        "    count = 0\n",
        "    test_list = [\"what do you want the thick pens\"]\n",
        "    #, \"she was quite happily pulling it\", \"<unk> these <unk> <unk> are these your water wings\", \"do you think #the policeman'll be after him\", \"that's <unk> that's food for the cat\", \"is it a big hat that\", \"are you #building it up or breaking it up\", \"there were two on there earlier , weren't there\", \"well I can't throw it #now , can I\", \"and what does a cow say\"]\n",
        "    #while len(test_list) < 10:\n",
        "    #    line = tensor_to_line(random.choice(train_data))\n",
        "    #    print(line)\n",
        "    #    if (len(line) > 5\n",
        "    #        and len(line) <10):\n",
        "    #        test_list.append(' '.join(line))\n",
        "    for line in test_list:\n",
        "        print(line)\n",
        "        output = translate_sentence(line, model, 'cuda', 50)\n",
        "        print('input', line)\n",
        "        print('output', output)\n",
        "        count += len(output)\n",
        "    print('word count', count)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPT9LEs1f9hd",
        "outputId": "681dcfff-8314-43f1-b6dc-31fd64cd540e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "def run_model(name_string):\n",
        "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "    CLIP = 1\n",
        "    print('out_dir', out_dir)\n",
        "    print('hid_dim', HID_DIM)\n",
        "    print('TFR', TFR)\n",
        "    print('epochs', N_EPOCHS)\n",
        "    best_valid_loss = float('inf')\n",
        "    MAX_LEN = 20\n",
        "    max_target = 200\n",
        "    name_string = name_string + str(HID_DIM) + '-50-ep-'\n",
        "    print('name_string', name_string)\n",
        "    for epoch in range(1, N_EPOCHS +1):\n",
        "        part_tr, part_va = [], []\n",
        "        while len(part_tr) < 8000:\n",
        "            part_tr.append(random.choice(train_data))\n",
        "        while len(part_va) < 2000:\n",
        "            part_va.append(random.choice(train_data))\n",
        "        print('tr', part_tr[0])\n",
        "        print('va', part_va[0])\n",
        "        #for i in range(len(part_tr)):\n",
        "        #    if len(part_tr[i][1]) > max_target:\n",
        "        #        part_tr[i] = [part_tr[i][0], truncate_random(part_tr[i][1], max=max_target)]\n",
        "        #        ##part_tr[i][1] = truncate_random(part_tr[i][1], max_target)\n",
        "        #        #print('done')\n",
        "        #        #print('in, out', part_tr[i][0], part_tr[i][1])\n",
        "        #train_iter = DataLoader(part_tr, batch_size=args.batch_size, shuffle=True, num_workers=4, **kwargs)\n",
        "        train_iter = DataLoader(part_tr, batch_size=BATCH_SIZE,\n",
        "                                shuffle=True, collate_fn=generate_batch)\n",
        "        valid_iter = DataLoader(part_va, batch_size=BATCH_SIZE,\n",
        "                                shuffle=True, collate_fn=generate_batch)\n",
        "        #test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
        "        #                       shuffle=True, collate_fn=generate_batch)\n",
        "\n",
        "\n",
        "        start_time = time.time()\n",
        "        train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
        "        valid_loss = evaluate(model, valid_iter, criterion)\n",
        "        del(part_tr)\n",
        "        del(part_va)\n",
        "        del(train_iter)\n",
        "        del(valid_iter)\n",
        "        end_time = time.time()\n",
        "\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        ##print('epoch', epoch+1, epoch_mins, epoch_secs)\n",
        "        print(f'Epoch: {epoch:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "        torch.cuda.empty_cache()\n",
        "        if (epoch > 0\n",
        "                and epoch % 10 == 0):\n",
        "            test_riches(name_string + str(epoch))\n",
        "            test_manc()\n",
        "            torch.save(model.state_dict(), out_dir + name_string + str(epoch) + '.pth')\n",
        "        #if (epoch > 0\n",
        "        #            and epoch % 50 == 0):\n",
        "        #    add_noise()\n",
        "\n",
        "N_EPOCHS = 500\n",
        "\n",
        "PRETRAINED = 'False'\n",
        "for HID_DIM in [500]:\n",
        "    #[250, 500]:\n",
        "        for i in range(1,6):\n",
        "            BATCH_SIZE = 10\n",
        "            print('TFR=', TFR)\n",
        "            print('run is', str(i))\n",
        "            name_string = 'pt-tf05-500-run-' + str(i) +'-wemb-06-07-'\n",
        "            enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT, embeddings_matrix)\n",
        "            ##enc.embedding.weight.data = embeddings_matrix\n",
        "            dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT, embeddings_matrix)\n",
        "            model = Seq2Seq(enc, dec, device).to(device)\n",
        "            model.apply(init_weights)\n",
        "            optimizer = optim.Adam(model.parameters())\n",
        "            load_embs()\n",
        "            print(model.eval())\n",
        "            run_model(name_string)\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TFR= 0.5\n",
            "run is 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-03bd0c0bc933>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'run is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mname_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'pt-tf05-500-run-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'-wemb-06-07-'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINPUT_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mENC_EMB_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHID_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_LAYERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mENC_DROPOUT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0;31m##enc.embedding.weight.data = embeddings_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEC_EMB_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHID_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_LAYERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEC_DROPOUT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'embeddings_matrix' is not defined"
          ]
        }
      ]
    }
  ]
}