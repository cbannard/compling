{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOIB3tcNWK1a2Bk9jsTNX90",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cbannard/compling/blob/main/seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEQrUh-KmOYE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23d92666-8fc6-4552-b9d6-02f83afbd37f"
      },
      "source": [
        "%reset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import Counter\n",
        "\n",
        "import torchtext\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "from torchtext.legacy.vocab import Vocab\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import platform\n",
        "import os\n",
        "import io\n",
        "import copy\n",
        "global enc, dec, model\n",
        "TFR = 0.5\n",
        "data_path = path = F\"/content/gdrive/My Drive/Modelling_Sentence_Repetition/Code/s2s/\" \n",
        "train_filepaths = [data_path + 'source-all.txt']\n",
        "val_filepaths = train_filepaths\n",
        "test_filepaths = train_filepaths\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpwfnS1aAie-"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZndFAuavmnl"
      },
      "source": [
        "def strip_string(str):\n",
        "    out = ''\n",
        "    for char in str:\n",
        "        if char not in ['“', '”', '[', ']', '<', '>']:\n",
        "            out = out + char\n",
        "    return out"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4hx-56luy3w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e13a731-62af-4c34-8ae7-837fbd21ff60"
      },
      "source": [
        "def get_riches_targets():\n",
        "    max_len = 0\n",
        "    print('getting Riches data')\n",
        "    outlist = []\n",
        "    with open( data_path + 'target-lines-Riches-punct.txt', encoding = 'utf-8') as f:\n",
        "        inlist = f.read().split('\\n')\n",
        "    for item in inlist:\n",
        "        index = item.find(',')\n",
        "        if index > 0:\n",
        "            outlist.append(item[index+1:].split())\n",
        "    for item in outlist:\n",
        "        if len(item) > max_len:\n",
        "            max_len = len(item)\n",
        "    print('max length is', max_len)\n",
        "    return outlist\n",
        "\n",
        "test_set = get_riches_targets()\n",
        "\n",
        "src_counts = {}"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "getting Riches data\n",
            "max length is 14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rryoJ9arerr9"
      },
      "source": [
        "def build_vocab(filepath, test_set):\n",
        "    counter = Counter()\n",
        "    with open(filepath, encoding=\"utf8\") as infile:\n",
        "        inlist = infile.read().splitlines()\n",
        "        print(len(inlist))\n",
        "        #for string_ in ['<pad>', '<bos>', '<eos>']:\n",
        "        #    for i in range(100):\n",
        "        #        counter.update(string_)\n",
        "        for line in inlist:\n",
        "            line = line.split()\n",
        "            for string_ in line:\n",
        "                ##print(string_)\n",
        "                ## just pass [string] instead of tokenizer\n",
        "                counter.update([strip_string(string_)])\n",
        "        for line in test_set:\n",
        "            for i in range(100):\n",
        "                for string_ in line:\n",
        "                    ##print(string_)\n",
        "                    counter.update(strip_string(string_))\n",
        "    return Vocab(counter, specials=['<unk>', '<bos>', '<eos>', '<pad>'])\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CX0jXeBd5dK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87ac08b1-4951-403e-de9a-efad2d8fc805"
      },
      "source": [
        "full_vocab = build_vocab(test_filepaths[0], test_set)\n",
        "src_vocab = Vocab(counter=full_vocab.freqs, min_freq=5, specials=['<unk>', '<bos>', '<eos>', '<pad>'])\n",
        "#trg_vocab = build_vocab(train_filepaths[1], trg_tokenizer)\n",
        "trg_vocab = src_vocab\n",
        "del(full_vocab)\n",
        "\n",
        "embedding_length = 50\n",
        "temp_matrix = {}\n",
        "embeddings_matrix = torch.zeros((len(src_vocab), 50))\n",
        "PAD_IDX = trg_vocab.stoi['<pad>']\n",
        "FUNCTION_WORDS = ['a', 'an', 'the',\n",
        "                  'and', 'but', 'with', 'of', 'not', 'as', 'with', 'at', 'more',\n",
        "                  'in', 'on', 'out', 'up', 'of', 'off', 'here', 'there',\n",
        "                 'I', 'you', 'we', 'they', 'me', 'us', 'them', 'mine',\n",
        "                  'yours', 'ours', 'theirs',\n",
        "                  'he', 'she', 'it', 'this', 'that', 'these', 'those', 'him', 'her',\n",
        "                  'is', 'are', 'be', 'was', 'were', \"I'm\", \"you're\", \"he's\", \"she's\",\n",
        "                  \"it's\", \"we're\", \"they're\", \"that's\", \"here's\", \"there's\",\n",
        "                  'have', 'can', 'will', 'had', 'could', 'would', \"can't\", \"won't\",\n",
        "                  'do', \"don't\", 'does'\n",
        "                                 'what', 'where', 'which', 'who', 'how']\n",
        "\n",
        "PRETRAINED = 'False'\n",
        "HID_DIM = 100\n",
        "N_LAYERS = 1\n",
        "ENC_DROPOUT = 0.0\n",
        "DEC_DROPOUT = 0.0\n",
        "INPUT_DIM = len(src_vocab)\n",
        "OUTPUT_DIM = len(trg_vocab)\n",
        "ENC_EMB_DIM = 50\n",
        "DEC_EMB_DIM = 50\n",
        "N_EPOCHS = 2\n",
        "EPOCH_SIZE = 10\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1015640\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqw5M3WcGJQW"
      },
      "source": [
        "#train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
        "#                                                  fields = (SRC, TRG))\n",
        "def data_process(filepaths):\n",
        "    MAX_LEN = 15\n",
        "    raw_src_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "    raw_trg_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "    data = []\n",
        "    for (raw_src, raw_trg) in zip(raw_src_iter, raw_trg_iter):\n",
        "        raw_src = raw_src.split()\n",
        "        for i in range(len(raw_src)):\n",
        "            raw_src[i] = strip_string(raw_src[i])\n",
        "        if  len(raw_src) < MAX_LEN:\n",
        "            #raw_src = ['<bos>'] + raw_src + ['<eos>']\n",
        "            #while len(raw_src) < MAX_LEN + 2:\n",
        "            ##    raw_src = ['<pad>'] + raw_src\n",
        "            #    raw_src = raw_src + ['<pad>']\n",
        "            #raw_src = ' '.join(raw_src)\n",
        "            #print(raw_src)\n",
        "            for i in range(len(raw_src)):\n",
        "                ##raw_src[i] = strip_string(raw_src[i])\n",
        "                if src_vocab.stoi[raw_src[i]] == 0:\n",
        "                    #print('unk', raw_src[i])\n",
        "                    int = random.randint(4, len(src_vocab) -1)\n",
        "                    #print(int)\n",
        "                    #print('random', src_vocab.itos[int])\n",
        "                    raw_src[i] = src_vocab.itos[int]\n",
        "            raw_trg = copy.deepcopy(raw_src)\n",
        "            #raw_src = ['<bos>'] + raw_src + ['<eos>']\n",
        "            raw_trg = ['<bos>'] + raw_trg + ['<eos>']\n",
        "            while len(raw_src) < MAX_LEN:\n",
        "                #raw_src = raw_src + ['<pad>']\n",
        "                raw_src = ['<pad>'] + raw_src\n",
        "                raw_trg = raw_trg +  ['<pad>']\n",
        "            #src_tensor_ = torch.tensor([src_vocab[token] for token in raw_src],\n",
        "            #                            #src_tokenizer(raw_src)],\n",
        "            #    dtype=torch.long)\n",
        "            src_tensor_ = torch.tensor([src_vocab[token] for token in raw_src], dtype=torch.long)\n",
        "            #print(len(src_tensor_))\n",
        "            if src_tensor_[-1] == src_vocab.stoi['\\n']:\n",
        "                src_tensor_ = src_tensor_[0:-1]\n",
        "            trg_tensor_ = torch.tensor([src_vocab[token] for token in raw_trg], dtype=torch.long)\n",
        "        #print(len(src_tensor_))\n",
        "            if trg_tensor_[-1] == src_vocab.stoi['\\n']:\n",
        "                trg_tensor_ = src_tensor_[0:-1]\n",
        "            #trg_tensor_ = torch.tensor([src_vocab[token] for token in src_tokenizer(raw_trg)],\n",
        "            #                           dtype=torch.long)\n",
        "            #if trg_tensor_[-1] == src_vocab.stoi['\\n']:\n",
        "            #    trg_tensor_ = trg_tensor_[0:-1]\n",
        "            data.append((src_tensor_, trg_tensor_))\n",
        "    return data\n",
        "    \n",
        "train_data = data_process(train_filepaths)\n",
        "val_data = data_process(val_filepaths)\n",
        "test_data = data_process(test_filepaths)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsmMhjY37xQY"
      },
      "source": [
        "def load_embs():\n",
        "    global enc, dec, model\n",
        "    print_emb('wants')\n",
        "    for file in ['enc-embs-1000.out', 'dec-embs-1000.out']:\n",
        "        with open(data_path + file, 'r', encoding = 'utf-8') as enc_file:\n",
        "            inlist = enc_file.read().splitlines()\n",
        "            print(len(inlist))\n",
        "            for line in inlist:\n",
        "                line = line.split(',')\n",
        "                word, emb = line[0], ' '.join(line[1:])\n",
        "                #print(word,emb)\n",
        "                if word not in FUNCTION_WORDS:\n",
        "                    emb = torch.tensor(np.fromstring(emb, sep = ' '))\n",
        "                    if 'enc-embs' in file:\n",
        "                        int = src_vocab.stoi[word]\n",
        "                        #print(word, len(word), int)\n",
        "                        if word == 'wants':\n",
        "                            print('wants', enc.embedding.weight.data[int])\n",
        "                        enc.embedding.weight.data[int] = emb\n",
        "                        if word == 'wants':\n",
        "                            print('enc', enc.embedding.weight.data[int])\n",
        "                    elif 'dec-embs' in file:\n",
        "                        ##print('dec')\n",
        "                        int = trg_vocab.stoi[word]\n",
        "                        dec.embedding.weight.data[int] = emb\n",
        "    print_emb('wants')\n",
        "    print_emb('is')\n",
        "\n",
        "def print_emb(word):\n",
        "    global enc\n",
        "    lookup = torch.tensor([src_vocab.stoi[word]], dtype=torch.long)\n",
        "    emb = enc.embedding.weight.data[lookup]\n",
        "    print(emb)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LQfrNOyYJDy"
      },
      "source": [
        "def generate_batch(data_batch):\n",
        "    #print('len in', len(data_batch))\n",
        "    #print(data_batch)\n",
        "    src_batch, trg_batch = [], []\n",
        "    for (src_item, trg_item) in data_batch:\n",
        "        ##### I think we only want to add beg and end markers to the target, not to the source...\n",
        "        #src_batch.append(src_item)\n",
        "        #trg_batch.append(trg_item)\n",
        "        src_batch.append(torch.cat([src_item], dim=0))\n",
        "        trg_batch.append(torch.cat([trg_item], dim=0))\n",
        "        #src_batch.append(torch.cat([torch.tensor([BOS_IDX]), src_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "        #trg_batch.append(torch.cat([torch.tensor([BOS_IDX]), trg_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX)\n",
        "    #print('len out', len(src_batch))\n",
        "    #print(src_batch)\n",
        "    return src_batch, trg_batch\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BacwRm7GztH"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kyBhDeqG6ZQ"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout, weights_matrix):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        if PRETRAINED == 'False':\n",
        "            self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx= PAD_IDX)\n",
        "        elif PRETRAINED == 'True':\n",
        "            print('encoder using trained embeddings')\n",
        "            self.embedding = nn.Embedding.from_pretrained(weights_matrix, freeze = 'False',padding_idx = PAD_IDX)\n",
        "\n",
        "        ##self.embedding = create_emb_layer(weights_matrix)\n",
        "        lookup = torch.tensor([src_vocab.stoi['baby']], dtype=torch.long)\n",
        "        emb = self.embedding(lookup)\n",
        "        #print(emb)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "\n",
        "        #src = [src len, batch size]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        #print('done')\n",
        "        #print(embedded[209])\n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "\n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        #outputs are always from the top hidden layer\n",
        "        ##print('cell', cell[0][0])\n",
        "        ## add random noise (mean 0, var 1) to the cell state\n",
        "        rndm = torch.randn(cell.size()).to(device)\n",
        "        #cell = cell + (0.5 * rndm)\n",
        "        #hidden = hidden + (0.5 * rndm)\n",
        "        #print('rand', rndm[0][0])\n",
        "        #print('new', cell[0][0])\n",
        "        #for name in Encoder.named_parameters(self):\n",
        "        #    if 'weight' in name[0]:\n",
        "        #        print('weights', name)\n",
        "        return hidden, cell\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtqBBcHEG_Q7"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout, weights_matrix):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        if PRETRAINED == 'False':\n",
        "            self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx = PAD_IDX)\n",
        "        elif PRETRAINED == 'True':\n",
        "            print('decoder using trained embeddings')\n",
        "            self.embedding = nn.Embedding.from_pretrained(weights_matrix, freeze = 'False',\n",
        "                                                         padding_idx= PAD_IDX)\n",
        "        ##self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        ##self.embedding = nn.Embedding.from_pretrained(weights_matrix, freeze = 'False')\n",
        "        lookup = torch.tensor([src_vocab.stoi['baby']], dtype=torch.long)\n",
        "        emb = self.embedding(lookup)\n",
        "        ##print(emb)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "\n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        #n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #context = [n layers, batch size, hid dim]\n",
        "\n",
        "        input = input.unsqueeze(0)\n",
        "\n",
        "        #input = [1, batch size]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "        #embedded = [1, batch size, emb dim]\n",
        "\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        #print('out', output)\n",
        "        #print('hid', hidden)\n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        rndm = torch.randn(cell.size()).to(device)\n",
        "        #cell = cell + (0.25 * rndm)\n",
        "        #hidden = hidden + (0.25 * rndm)\n",
        "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #cell = [n layers, batch size, hid dim]\n",
        "\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "\n",
        "        #prediction = [batch size, output dim]\n",
        "\n",
        "        return prediction, hidden, cell\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlfJE5HsI-az"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        #print(\"TFR =\", teacher_forcing_ratio)\n",
        "        #print('fw src', src)\n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "\n",
        "            #insert input token embedding, previous hidden and previous cell states\n",
        "            #receive output tensor (predictions) and new hidden and cell states\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "\n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "\n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            #print('tf', teacher_force)\n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1)\n",
        "\n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjeOtxOSJae_"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "def train(model: nn.Module,\n",
        "          iterator: torch.utils.data.DataLoader,\n",
        "          optimizer: optim.Optimizer,\n",
        "          criterion: nn.Module,\n",
        "          clip: float):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for _, (src, trg) in enumerate(iterator):\n",
        "        ##print('src', src)\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg)\n",
        "\n",
        "        output = output[1:].view(-1, output.shape[-1])\n",
        "        trg = trg[1:].view(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sqk12W2Jfhr"
      },
      "source": [
        "def evaluate(model: nn.Module,\n",
        "             iterator: torch.utils.data.DataLoader,\n",
        "             criterion: nn.Module):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for _, (src, trg) in enumerate(iterator):\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            output = output[1:].view(-1, output.shape[-1])\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yopqp1bdJqmL"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n85PqMu2bB0I"
      },
      "source": [
        "def test_manc():\n",
        "    test_list = []\n",
        "    count = 0\n",
        "    test_list = [\"what do you want the thick pens\"]\n",
        "    #, \"she was quite happily pulling it\", \"<unk> these <unk> <unk> are these your water wings\", \"do you think #the policeman'll be after him\", \"that's <unk> that's food for the cat\", \"is it a big hat that\", \"are you #building it up or breaking it up\", \"there were two on there earlier , weren't there\", \"well I can't throw it #now , can I\", \"and what does a cow say\"]\n",
        "    #while len(test_list) < 10:\n",
        "    #    line = tensor_to_line(random.choice(train_data))\n",
        "    #    print(line)\n",
        "    #    if (len(line) > 5\n",
        "    #        and len(line) <10):\n",
        "    #        test_list.append(' '.join(line))\n",
        "    for line in test_list:\n",
        "        print(line)\n",
        "        output = translate_sentence(line, model, 'cuda', 50)\n",
        "        print('input', line)\n",
        "        print('output', output)\n",
        "        count += len(output)\n",
        "    print('word count', count)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jj7JQFDmHz_j"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPT9LEs1f9hd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "8ba60a24-6d72-471e-ca87-14fb2303adc7"
      },
      "source": [
        "out_dir = data_path + \"saved-models/\"\n",
        "\n",
        "def count_parameters(model: nn.Module):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def run_model(name_string):\n",
        "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "    CLIP = 1\n",
        "    print('out_dir', out_dir)\n",
        "    print('hid_dim', HID_DIM)\n",
        "    print('TFR', TFR)\n",
        "    print('epochs', N_EPOCHS)\n",
        "    best_valid_loss = float('inf')\n",
        "    MAX_LEN = 20\n",
        "    max_target = 200\n",
        "    name_string = name_string + str(HID_DIM) + '-50-ep-'\n",
        "    print('name_string', name_string)\n",
        "    for epoch in range(1, N_EPOCHS +1):\n",
        "        part_tr, part_va = [], []\n",
        "        while len(part_tr) < 8000:\n",
        "            part_tr.append(random.choice(train_data))\n",
        "        while len(part_va) < 2000:\n",
        "            part_va.append(random.choice(train_data))\n",
        "        print('tr', part_tr[0])\n",
        "        print('va', part_va[0])\n",
        "        #for i in range(len(part_tr)):\n",
        "        #    if len(part_tr[i][1]) > max_target:\n",
        "        #        part_tr[i] = [part_tr[i][0], truncate_random(part_tr[i][1], max=max_target)]\n",
        "        #        ##part_tr[i][1] = truncate_random(part_tr[i][1], max_target)\n",
        "        #        #print('done')\n",
        "        #        #print('in, out', part_tr[i][0], part_tr[i][1])\n",
        "        #train_iter = DataLoader(part_tr, batch_size=args.batch_size, shuffle=True, num_workers=4, **kwargs)\n",
        "        train_iter = DataLoader(part_tr, batch_size=BATCH_SIZE,\n",
        "                                shuffle=True, collate_fn=generate_batch)\n",
        "        valid_iter = DataLoader(part_va, batch_size=BATCH_SIZE,\n",
        "                                shuffle=True, collate_fn=generate_batch)\n",
        "        #test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
        "        #                       shuffle=True, collate_fn=generate_batch)\n",
        "\n",
        "\n",
        "        start_time = time.time()\n",
        "        train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
        "        valid_loss = evaluate(model, valid_iter, criterion)\n",
        "        del(part_tr)\n",
        "        del(part_va)\n",
        "        del(train_iter)\n",
        "        del(valid_iter)\n",
        "        end_time = time.time()\n",
        "\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        ##print('epoch', epoch+1, epoch_mins, epoch_secs)\n",
        "        print(f'Epoch: {epoch:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "        torch.cuda.empty_cache()\n",
        "        if (epoch > 0\n",
        "                and epoch % 10 == 0):\n",
        "            test_riches(name_string + str(epoch))\n",
        "            test_manc()\n",
        "            torch.save(model.state_dict(), out_dir + name_string + str(epoch) + '.pth')\n",
        "        #if (epoch > 0\n",
        "        #            and epoch % 50 == 0):\n",
        "        #    add_noise()\n",
        "\n",
        "N_EPOCHS = 2\n",
        "\n",
        "PRETRAINED = 'False'\n",
        "for HID_DIM in [500]:\n",
        "    #[250, 500]:\n",
        "        for i in range(1,6):\n",
        "            BATCH_SIZE = 10\n",
        "            print('TFR=', TFR)\n",
        "            print('run is', str(i))\n",
        "            name_string = 'pt-tf05-500-run-' + str(i) +'-wemb-06-07-'\n",
        "            enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT, embeddings_matrix)\n",
        "            ##enc.embedding.weight.data = embeddings_matrix\n",
        "            dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT, embeddings_matrix)\n",
        "            model = Seq2Seq(enc, dec, device).to(device)\n",
        "            model.apply(init_weights)\n",
        "            optimizer = optim.Adam(model.parameters())\n",
        "            load_embs()\n",
        "            print(model.eval())\n",
        "            run_model(name_string)\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TFR= 0.5\n",
            "run is 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-c88f92b976f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEC_EMB_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHID_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_LAYERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEC_DROPOUT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeq2Seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mload_embs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'init_weights' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eAVclZEagT0"
      },
      "source": [
        "def translate_sentence(sentence = \"there were two on there earlier , weren't there\", model = model, device = 'cuda', max_len = 50):\n",
        "    ##sentence = \"there goes a man\"\n",
        "    #print(\"INPUT:\", sentence)\n",
        "    ##sentence = 'hoe gaat het nu met je'\n",
        "    ##print('sentence', sentence)\n",
        "    #if isinstance(sentence, str):\n",
        "    #    sentence = '<bos> ' + sentence + ' <eos>'\n",
        "    #    #sentence = sentence.split()\n",
        "    #else:\n",
        "    #    sentence = ['<bos>' + sentence + '<eos']\n",
        "    #    #sentence = sentence.split()\n",
        "    #pad out to max length + 2\n",
        "    sentence = sentence.split()\n",
        "    while len(sentence) < 15:\n",
        "    #    ## pad at right. This is what pad sequences does.\n",
        "    #    sentence = sentence + ['<pad>']\n",
        "       sentence = ['<pad>'] + sentence\n",
        "    tokens = [token.lower() for token in sentence]\n",
        "    #print(tokens)\n",
        "    src_indexes = [src_vocab.stoi[token] for token in tokens]\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "    ##print(src_indexes)\n",
        "    #print(src_tensor)\n",
        "    src_len = torch.LongTensor([len(src_indexes)])\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(src_tensor)\n",
        "    #print('hidden', hidden)\n",
        "    #print('cell', cell)\n",
        "    ##mask = model.create_mask(src_tensor)\n",
        "    trg_indexes = [trg_vocab.stoi['<bos>']]\n",
        "    output_indexes = []\n",
        "    #attentions = torch.zeros(max_len, 1, len(src_indexes)).to(device)\n",
        "    ##cell = torch.zeros(2,1,512).to(device)\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "        #print('trg_tensor', trg_tensor)\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(trg_tensor, hidden.to(device), cell.to(device))\n",
        "        #print('out', output, len(output[0]))\n",
        "        res, ind = output.topk(3)\n",
        "\n",
        "        m = nn.Softmax(dim=1)\n",
        "        temp = m(output)\n",
        "        prob, ind2 = temp.topk(3)\n",
        "        prop = prob[0][0]\n",
        "        ##print(prob)\n",
        "        ##print(res, len(res), ind)\n",
        "        #print(trg_vocab.itos[ind[0][0].item()], trg_vocab.itos[ind[0][1].item()], trg_vocab.itos[ind[0][2].item()])\n",
        "        #print('activations', round(res[0][0].item(),2), round(res[0][1].item(),2),\n",
        "        #      round(res[0][2].item(),2))\n",
        "        #prop = round(res[0][0].item()/(res[0][1].item() + res[0][2].item()), 2)\n",
        "        #if trg_vocab.itos[ind[0][0]] in FUNCTION_WORDS:\n",
        "        #    print('fword')\n",
        "        if (trg_vocab.itos[ind[0][0]] in FUNCTION_WORDS\n",
        "            and prop > 0.70):\n",
        "            #print('adding fword')\n",
        "            pred_token = output.argmax(1).item()\n",
        "        elif (trg_vocab.itos[ind[0][0]] not in FUNCTION_WORDS\n",
        "            and prop > 0.6):\n",
        "            pred_token = output.argmax(1).item()\n",
        "        elif trg_vocab.itos[output.argmax(1).item()] == '<eos>':\n",
        "            pred_token = output.argmax(1).item()\n",
        "        else:\n",
        "            pred_token = trg_vocab.stoi['<pad>']\n",
        "\n",
        "        ##pred_token = output.argmax(1).item()\n",
        "        ###here we feed in the actual model prediction, rather than the padding token'\n",
        "        ### but we collect the padding token for production\n",
        "        trg_indexes.append(output.argmax(1).item())\n",
        "        #trg_indexes.append(pred_token)\n",
        "        if trg_vocab.itos[pred_token] != '<bos>':\n",
        "            output_indexes.append(pred_token)\n",
        "\n",
        "        #print('target word =', sentence[i+1])\n",
        "        #print('candidates', trg_vocab.itos[ind[0][0]], trg_vocab.itos[ind[0][1]], trg_vocab.itos[ind[0][2]])\n",
        "        #print('prop', prop)\n",
        "        #print('pred', trg_vocab.itos[pred_token], pred_token)\n",
        "        #print('feeding back', trg_indexes)\n",
        "        #print('feeding', [trg_vocab.itos[i] for i in trg_indexes])\n",
        "        #print('pred', pred_token, trg_vocab.itos[pred_token])\n",
        "        #print('actual pred', output.argmax(1).item(), trg_vocab.itos[output.argmax(1).item()])\n",
        "\n",
        "        #print('feeding', [trg_vocab.itos[i] for i in trg_indexes])\n",
        "        #print('output indexes', output_indexes)\n",
        "        if pred_token == trg_vocab.stoi['<eos>']:\n",
        "            break\n",
        "    #print('getting output', output_indexes)\n",
        "    output_tokens = [trg_vocab.itos[i] for i in output_indexes]\n",
        "    #print(\"OUTPUT\", trg_tokens[1:])\n",
        "    ##print('output', output_tokens)\n",
        "    return(output_tokens)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}