{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMlF1SoJsvvApXtjctUpaO+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cbannard/compling/blob/main/seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEQrUh-KmOYE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b43e76d1-0a9c-477a-a0c7-9feff7cff645"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "\n",
        "import torchtext\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "from torchtext.legacy.vocab import Vocab\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import platform\n",
        "import os\n",
        "import io\n",
        "import copy\n",
        "global enc, dec, model\n",
        "TFR = 0.5\n",
        "data_path = path = F\"/content/gdrive/My Drive/Modelling_Sentence_Repetition/Code/s2s/\" \n",
        "train_filepaths = [data_path + 'source-all.txt']\n",
        "val_filepaths = train_filepaths\n",
        "test_filepaths = train_filepaths\n"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpwfnS1aAie-"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZO_7kITApQv"
      },
      "source": [
        "SRC = Field(tokenize = word_tokenize, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)\n",
        "\n",
        "TRG = Field(word_tokenize, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqw5M3WcGJQW"
      },
      "source": [
        "#train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
        "#                                                  fields = (SRC, TRG))\n",
        "def data_process(filepaths):\n",
        "    MAX_LEN = 15\n",
        "    raw_src_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "    raw_trg_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "    data = []\n",
        "    for (raw_src, raw_trg) in zip(raw_src_iter, raw_trg_iter):\n",
        "        raw_src = raw_src.split()\n",
        "        for i in range(len(raw_src)):\n",
        "            raw_src[i] = strip_string(raw_src[i])\n",
        "        if  len(raw_src) < MAX_LEN:\n",
        "            #raw_src = ['<bos>'] + raw_src + ['<eos>']\n",
        "            #while len(raw_src) < MAX_LEN + 2:\n",
        "            ##    raw_src = ['<pad>'] + raw_src\n",
        "            #    raw_src = raw_src + ['<pad>']\n",
        "            #raw_src = ' '.join(raw_src)\n",
        "            #print(raw_src)\n",
        "            for i in range(len(raw_src)):\n",
        "                ##raw_src[i] = strip_string(raw_src[i])\n",
        "                if src_vocab.stoi[raw_src[i]] == 0:\n",
        "                    #print('unk', raw_src[i])\n",
        "                    int = random.randint(4, len(src_vocab) -1)\n",
        "                    #print(int)\n",
        "                    #print('random', src_vocab.itos[int])\n",
        "                    raw_src[i] = src_vocab.itos[int]\n",
        "            raw_trg = copy.deepcopy(raw_src)\n",
        "            #raw_src = ['<bos>'] + raw_src + ['<eos>']\n",
        "            raw_trg = ['<bos>'] + raw_trg + ['<eos>']\n",
        "            while len(raw_src) < MAX_LEN:\n",
        "                #raw_src = raw_src + ['<pad>']\n",
        "                raw_src = ['<pad>'] + raw_src\n",
        "                raw_trg = raw_trg +  ['<pad>']\n",
        "            #src_tensor_ = torch.tensor([src_vocab[token] for token in raw_src],\n",
        "            #                            #src_tokenizer(raw_src)],\n",
        "            #    dtype=torch.long)\n",
        "            src_tensor_ = torch.tensor([src_vocab[token] for token in raw_src], dtype=torch.long)\n",
        "            #print(len(src_tensor_))\n",
        "            if src_tensor_[-1] == src_vocab.stoi['\\n']:\n",
        "                src_tensor_ = src_tensor_[0:-1]\n",
        "            trg_tensor_ = torch.tensor([src_vocab[token] for token in raw_trg], dtype=torch.long)\n",
        "        #print(len(src_tensor_))\n",
        "            if trg_tensor_[-1] == src_vocab.stoi['\\n']:\n",
        "                trg_tensor_ = src_tensor_[0:-1]\n",
        "            #trg_tensor_ = torch.tensor([src_vocab[token] for token in src_tokenizer(raw_trg)],\n",
        "            #                           dtype=torch.long)\n",
        "            #if trg_tensor_[-1] == src_vocab.stoi['\\n']:\n",
        "            #    trg_tensor_ = trg_tensor_[0:-1]\n",
        "            data.append((src_tensor_, trg_tensor_))\n",
        "    return data\n",
        "    \n",
        "train_data = data_process(train_filepaths)\n",
        "val_data = data_process(val_filepaths)\n",
        "test_data = data_process(test_filepaths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTaxjgvbGxlt"
      },
      "source": [
        "#SRC.build_vocab(train_data, min_freq = 2)\n",
        "#TRG.build_vocab(train_data, min_freq = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZndFAuavmnl"
      },
      "source": [
        "def strip_string(str):\n",
        "    out = ''\n",
        "    for char in str:\n",
        "        if char not in ['“', '”', '[', ']', '<', '>']:\n",
        "            out = out + char\n",
        "    return out"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4hx-56luy3w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4f0b12d-4e23-4325-eb56-24252c7a438d"
      },
      "source": [
        "def get_riches_targets():\n",
        "    max_len = 0\n",
        "    print('getting Riches data')\n",
        "    outlist = []\n",
        "    with open( data_path + 'target-lines-Riches-punct.txt', encoding = 'utf-8') as f:\n",
        "        inlist = f.read().split('\\n')\n",
        "    for item in inlist:\n",
        "        index = item.find(',')\n",
        "        if index > 0:\n",
        "            outlist.append(item[index+1:].split())\n",
        "    for item in outlist:\n",
        "        if len(item) > max_len:\n",
        "            max_len = len(item)\n",
        "    print('max length is', max_len)\n",
        "    return outlist\n",
        "\n",
        "test_set = get_riches_targets()\n",
        "\n",
        "src_counts = {}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "getting Riches data\n",
            "max length is 14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rryoJ9arerr9"
      },
      "source": [
        "def build_vocab(filepath, test_set):\n",
        "    counter = Counter()\n",
        "    with open(filepath, encoding=\"utf8\") as infile:\n",
        "        inlist = infile.read().splitlines()\n",
        "        print(len(inlist))\n",
        "        #for string_ in ['<pad>', '<bos>', '<eos>']:\n",
        "        #    for i in range(100):\n",
        "        #        counter.update(string_)\n",
        "        for line in inlist:\n",
        "            line = line.split()\n",
        "            for string_ in line:\n",
        "                ##print(string_)\n",
        "                ## just pass [string] instead of tokenizer\n",
        "                counter.update([strip_string(string_)])\n",
        "        for line in test_set:\n",
        "            for i in range(100):\n",
        "                for string_ in line:\n",
        "                    ##print(string_)\n",
        "                    counter.update(strip_string(string_))\n",
        "    return Vocab(counter, specials=['<unk>', '<bos>', '<eos>', '<pad>'])\n"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CX0jXeBd5dK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbfa348e-7adb-4c07-b3ed-2e6cb8c7d78b"
      },
      "source": [
        "full_vocab = build_vocab(test_filepaths[0], test_set)\n",
        "src_vocab = Vocab(counter=full_vocab.freqs, min_freq=5, specials=['<unk>', '<bos>', '<eos>', '<pad>'])\n",
        "#trg_vocab = build_vocab(train_filepaths[1], trg_tokenizer)\n",
        "trg_vocab = src_vocab\n",
        "del(full_vocab)\n",
        "\n",
        "embedding_length = 50\n",
        "temp_matrix = {}\n",
        "embeddings_matrix = torch.zeros((len(src_vocab), 50))\n",
        "PAD_IDX = trg_vocab.stoi['<pad>']\n",
        "FUNCTION_WORDS = ['a', 'an', 'the',\n",
        "                  'and', 'but', 'with', 'of', 'not', 'as', 'with', 'at', 'more',\n",
        "                  'in', 'on', 'out', 'up', 'of', 'off', 'here', 'there',\n",
        "                 'I', 'you', 'we', 'they', 'me', 'us', 'them', 'mine',\n",
        "                  'yours', 'ours', 'theirs',\n",
        "                  'he', 'she', 'it', 'this', 'that', 'these', 'those', 'him', 'her',\n",
        "                  'is', 'are', 'be', 'was', 'were', \"I'm\", \"you're\", \"he's\", \"she's\",\n",
        "                  \"it's\", \"we're\", \"they're\", \"that's\", \"here's\", \"there's\",\n",
        "                  'have', 'can', 'will', 'had', 'could', 'would', \"can't\", \"won't\",\n",
        "                  'do', \"don't\", 'does'\n",
        "                                 'what', 'where', 'which', 'who', 'how']\n"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1015640\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsmMhjY37xQY"
      },
      "source": [
        "def load_embs():\n",
        "    global enc, dec, model\n",
        "    print_emb('wants')\n",
        "    for file in ['enc-embs-1000.out', 'dec-embs-1000.out']:\n",
        "        with open(data_path + file, 'r', encoding = 'utf-8') as enc_file:\n",
        "            inlist = enc_file.read().splitlines()\n",
        "            print(len(inlist))\n",
        "            for line in inlist:\n",
        "                line = line.split(',')\n",
        "                word, emb = line[0], ' '.join(line[1:])\n",
        "                #print(word,emb)\n",
        "                if word not in FUNCTION_WORDS:\n",
        "                    emb = torch.tensor(np.fromstring(emb, sep = ' '))\n",
        "                    if 'enc-embs' in file:\n",
        "                        int = src_vocab.stoi[word]\n",
        "                        #print(word, len(word), int)\n",
        "                        if word == 'wants':\n",
        "                            print('wants', enc.embedding.weight.data[int])\n",
        "                        enc.embedding.weight.data[int] = emb\n",
        "                        if word == 'wants':\n",
        "                            print('enc', enc.embedding.weight.data[int])\n",
        "                    elif 'dec-embs' in file:\n",
        "                        ##print('dec')\n",
        "                        int = trg_vocab.stoi[word]\n",
        "                        dec.embedding.weight.data[int] = emb\n",
        "    print_emb('wants')\n",
        "    print_emb('is')\n",
        "\n",
        "def print_emb(word):\n",
        "    global enc\n",
        "    lookup = torch.tensor([src_vocab.stoi[word]], dtype=torch.long)\n",
        "    emb = enc.embedding.weight.data[lookup]\n",
        "    print(emb)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BacwRm7GztH"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4vcMn2IMKVC"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE, \n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kyBhDeqG6ZQ"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout, weights_matrix):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        if PRETRAINED == 'False':\n",
        "            self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx= PAD_IDX)\n",
        "        elif PRETRAINED == 'True':\n",
        "            print('encoder using trained embeddings')\n",
        "            self.embedding = nn.Embedding.from_pretrained(weights_matrix, freeze = 'False',padding_idx = PAD_IDX)\n",
        "\n",
        "        ##self.embedding = create_emb_layer(weights_matrix)\n",
        "        lookup = torch.tensor([src_vocab.stoi['baby']], dtype=torch.long)\n",
        "        emb = self.embedding(lookup)\n",
        "        #print(emb)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "\n",
        "        #src = [src len, batch size]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        #print('done')\n",
        "        #print(embedded[209])\n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "\n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        #outputs are always from the top hidden layer\n",
        "        ##print('cell', cell[0][0])\n",
        "        ## add random noise (mean 0, var 1) to the cell state\n",
        "        rndm = torch.randn(cell.size()).to(device)\n",
        "        #cell = cell + (0.5 * rndm)\n",
        "        #hidden = hidden + (0.5 * rndm)\n",
        "        #print('rand', rndm[0][0])\n",
        "        #print('new', cell[0][0])\n",
        "        #for name in Encoder.named_parameters(self):\n",
        "        #    if 'weight' in name[0]:\n",
        "        #        print('weights', name)\n",
        "        return hidden, cell\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtqBBcHEG_Q7"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout, weights_matrix):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        if PRETRAINED == 'False':\n",
        "            self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx = PAD_IDX)\n",
        "        elif PRETRAINED == 'True':\n",
        "            print('decoder using trained embeddings')\n",
        "            self.embedding = nn.Embedding.from_pretrained(weights_matrix, freeze = 'False',\n",
        "                                                         padding_idx= PAD_IDX)\n",
        "        ##self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        ##self.embedding = nn.Embedding.from_pretrained(weights_matrix, freeze = 'False')\n",
        "        lookup = torch.tensor([src_vocab.stoi['baby']], dtype=torch.long)\n",
        "        emb = self.embedding(lookup)\n",
        "        ##print(emb)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "\n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        #n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #context = [n layers, batch size, hid dim]\n",
        "\n",
        "        input = input.unsqueeze(0)\n",
        "\n",
        "        #input = [1, batch size]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "        #embedded = [1, batch size, emb dim]\n",
        "\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        #print('out', output)\n",
        "        #print('hid', hidden)\n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        rndm = torch.randn(cell.size()).to(device)\n",
        "        #cell = cell + (0.25 * rndm)\n",
        "        #hidden = hidden + (0.25 * rndm)\n",
        "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #cell = [n layers, batch size, hid dim]\n",
        "\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "\n",
        "        #prediction = [batch size, output dim]\n",
        "\n",
        "        return prediction, hidden, cell\n"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlfJE5HsI-az"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        #print(\"TFR =\", teacher_forcing_ratio)\n",
        "        #print('fw src', src)\n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "\n",
        "            #insert input token embedding, previous hidden and previous cell states\n",
        "            #receive output tensor (predictions) and new hidden and cell states\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "\n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "\n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            #print('tf', teacher_force)\n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1)\n",
        "\n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-omtzS2JNQR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "965aba3f-4e72-4c8b-cabb-1bb2bf0a9b53"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(7845, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(7704, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (fc_out): Linear(in_features=512, out_features=7704, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMNjhdsOJXiy"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1ZXYDhnJZzP"
      },
      "source": [
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjeOtxOSJae_"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg)\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sqk12W2Jfhr"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            print(output)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yopqp1bdJqmL"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iftad7XbJrP9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0b21bee-312b-4114-d9c4-c0eed4312616"
      },
      "source": [
        "N_EPOCHS = 3\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 27m 18s\n",
            "\tTrain Loss: 3.297 | Train PPL:  27.020\n",
            "\t Val. Loss: 3.952 |  Val. PPL:  52.044\n",
            "Epoch: 02 | Time: 27m 3s\n",
            "\tTrain Loss: 3.170 | Train PPL:  23.815\n",
            "\t Val. Loss: 3.906 |  Val. PPL:  49.704\n",
            "Epoch: 03 | Time: 27m 12s\n",
            "\tTrain Loss: 3.027 | Train PPL:  20.643\n",
            "\t Val. Loss: 3.852 |  Val. PPL:  47.075\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDrsCNM0Snog",
        "outputId": "af1fe3fe-6926-41df-deeb-1ba14b27e51c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLpR8V3zTGKR"
      },
      "source": [
        "model.load_state_dict(torch.load('tut1-model.pt'))\n",
        "path = F\"/content/gdrive/My Drive/tut1-model.pt\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOlV-Q5dJztc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b451e60e-d2f6-459a-83db-84add172d16c"
      },
      "source": [
        "model.load_state_dict(torch.load('tut1-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 5.5188, -7.3904, -6.5965,  ..., -7.2304, -7.2678, -6.5158],\n",
            "        [ 5.4187, -9.1561, -7.5463,  ..., -6.0830, -6.7973, -5.3052],\n",
            "        [ 5.8970, -5.9536, -5.3926,  ..., -6.4508, -4.7705, -4.9962],\n",
            "        ...,\n",
            "        [ 5.8009, -7.6231, -5.7352,  ..., -5.5331, -6.5965, -7.5182],\n",
            "        [ 6.3387, -7.9489, -6.4208,  ..., -4.3347, -6.3744, -7.8730],\n",
            "        [ 5.6698, -7.6268, -6.5530,  ..., -3.9665, -6.0154, -8.7844]])\n",
            "tensor([[ 4.0191, -5.2919, -5.8278,  ..., -5.0277, -5.4157, -4.4312],\n",
            "        [ 4.2264, -8.3667, -7.5391,  ..., -8.3577, -7.7305, -6.1178],\n",
            "        [ 4.6301, -6.9763, -6.3382,  ..., -7.4339, -9.2199, -5.2434],\n",
            "        ...,\n",
            "        [ 6.0032, -8.5511, -7.4861,  ..., -5.8049, -6.6813, -5.7271],\n",
            "        [ 6.4811, -7.9516, -7.2906,  ..., -3.5158, -7.8159, -6.0269],\n",
            "        [ 5.5195, -7.1183, -6.4749,  ..., -3.6279, -3.7435, -7.5254]])\n",
            "tensor([[ 4.9541, -7.5637, -6.3644,  ..., -6.3101, -5.9431, -6.5570],\n",
            "        [ 3.6040, -7.2824, -7.9983,  ..., -6.8790, -6.8412, -5.7228],\n",
            "        [ 3.8169, -5.5160, -6.3860,  ..., -5.4438, -4.2568, -4.8005],\n",
            "        ...,\n",
            "        [ 7.0523, -9.3425, -7.8246,  ..., -2.9418, -6.0582, -4.3268],\n",
            "        [ 6.8716, -9.0659, -7.7854,  ...,  0.1320, -3.0601, -6.3847],\n",
            "        [ 7.2237, -7.6890, -5.9473,  ..., -4.1908, -8.2546, -5.8092]])\n",
            "tensor([[ 5.1051, -7.2938, -6.3933,  ..., -7.6528, -7.9204, -6.6670],\n",
            "        [ 4.5599, -7.4857, -6.3887,  ..., -6.8594, -6.2958, -5.8023],\n",
            "        [ 4.6133, -7.9587, -6.8600,  ..., -8.4796, -7.9089, -5.5260],\n",
            "        ...,\n",
            "        [ 6.2987, -6.8542, -6.1563,  ..., -4.1107, -5.7796, -9.4463],\n",
            "        [ 5.9095, -8.1677, -7.5952,  ..., -2.5369, -5.8357, -5.1233],\n",
            "        [ 5.8762, -7.1683, -6.3690,  ..., -3.8591, -7.8161, -7.5175]])\n",
            "tensor([[ 4.0636, -8.4637, -7.4564,  ..., -8.0659, -7.3694, -6.5137],\n",
            "        [ 4.4225, -6.8586, -5.7681,  ..., -6.4042, -6.1807, -5.0019],\n",
            "        [ 4.0326, -5.1474, -4.9530,  ..., -4.4405, -4.5268, -3.4727],\n",
            "        ...,\n",
            "        [ 6.1956, -7.9713, -7.3975,  ..., -3.5797, -8.2648, -4.6284],\n",
            "        [ 6.0103, -7.5520, -6.6029,  ..., -0.7089, -3.9905, -5.8914],\n",
            "        [ 6.8767, -9.2894, -7.1573,  ..., -5.6021, -6.8928, -7.5880]])\n",
            "tensor([[ 5.7379, -6.0032, -4.9610,  ..., -6.1169, -6.5529, -3.6303],\n",
            "        [ 5.1873, -6.2257, -5.1588,  ..., -6.2993, -5.5652, -3.4068],\n",
            "        [ 4.5045, -6.9700, -5.6093,  ..., -6.9090, -5.5698, -3.9392],\n",
            "        ...,\n",
            "        [ 5.8661, -8.8091, -7.0519,  ..., -4.0393, -6.7393, -3.6927],\n",
            "        [ 5.9090, -8.0460, -6.8996,  ..., -3.5376, -6.8182, -7.0000],\n",
            "        [ 5.1176, -7.9037, -6.9361,  ..., -5.5539, -5.0024, -5.2062]])\n",
            "tensor([[ 5.3522, -6.6183, -5.9904,  ..., -6.8393, -5.2003, -5.1687],\n",
            "        [ 5.7469, -6.1850, -5.4597,  ..., -5.4655, -6.7030, -3.5900],\n",
            "        [ 4.5189, -6.8783, -6.2829,  ..., -6.4825, -9.4307, -2.8198],\n",
            "        ...,\n",
            "        [ 5.2847, -8.3636, -7.4839,  ..., -3.2939, -5.4542, -5.9000],\n",
            "        [ 5.4156, -7.3027, -5.9829,  ..., -4.5984, -6.8312, -6.4398],\n",
            "        [ 5.7918, -7.8511, -6.4219,  ..., -4.1575, -6.5244, -6.6003]])\n",
            "tensor([[ 4.9804, -7.4917, -6.5634,  ..., -8.1453, -7.7005, -6.6131],\n",
            "        [ 5.0811, -7.4036, -6.3841,  ..., -8.3398, -8.0809, -6.5302],\n",
            "        [ 4.7319, -7.5819, -6.6389,  ..., -6.3652, -6.5201, -3.0499],\n",
            "        ...,\n",
            "        [ 5.4028, -7.6582, -7.2526,  ..., -3.6129, -5.3162, -6.7542],\n",
            "        [ 4.8383, -7.9668, -6.3518,  ..., -4.9865, -6.0849, -6.5189],\n",
            "        [ 5.3941, -8.0433, -7.2537,  ..., -4.0890, -5.8914, -6.4809]])\n",
            "| Test Loss: 3.879 | Test PPL:  48.371 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eAVclZEagT0"
      },
      "source": [
        "def translate_sentence(sentence = 'and what does a cow say', model = model, device = 'cuda', max_len = 50):\n",
        " \n",
        "    sentence = \"there were two on there earlier , weren't there\"\n",
        "    print(\"INPUT:\", sentence)\n",
        "    #if isinstance(sentence, str):\n",
        "    #    sentence = '<bos> ' + sentence + ' <eos>'\n",
        "    #    sentence = sentence.split()\n",
        "    #else:\n",
        "    #    sentence = ['<bos>' + sentence + '<eos']\n",
        "    #    sentence = sentence.split()\n",
        "    sentence =sentence.split()\n",
        "  \n",
        "    tokens = [token.lower() for token in sentence]\n",
        "    #print(tokens)\n",
        "    src_indexes = [src_vocab.stoi[token] for token in tokens]\n",
        "    ##print(src_indexes)\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "    ##print(src_tensor)\n",
        "    src_len = torch.LongTensor([len(src_indexes)])\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(src_tensor)\n",
        "    print('hidden', hidden)\n",
        "    ##mask = model.create_mask(src_tensor)\n",
        "    trg_indexes = [trg_vocab.stoi['<bos>']]\n",
        "    output_indexes = []\n",
        "\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(trg_tensor, hidden.to(device), cell.to(device))\n",
        "        \n",
        "        res, ind = output.topk(3)\n",
        "        print(res, len(res), ind)\n",
        "\n",
        "        for k in range(len(res[0])):\n",
        "            print('0', res[0][k])\n",
        "            print('1', k, res[0][k].item())\n",
        "            print('2', k, ind[0][k].item())\n",
        "            print('3', k, trg_vocab.itos[ind[0][k].item()])\n",
        "        print('4', (res[0][0])/(res[0][1]) + res[0][2])\n",
        "        print('5', res[0][0].item()/(res[0][1].item()/res[0][2].item()))\n",
        "\n",
        "        if (res[0][0] > 0.60 * (res[0][1] + res[0][2])\n",
        "            or trg_vocab.itos[output.argmax(1).item()] == '<eos>'):\n",
        "            pred_token = output.argmax(1).item()\n",
        "        else:\n",
        "            pred_token = trg_vocab.stoi['<pad>']\n",
        "        print('pred', pred_token)\n",
        "\n",
        "        trg_indexes.append(output.argmax(1).item())\n",
        "\n",
        "        output_indexes.append(pred_token)\n",
        "     \n",
        "        if pred_token == trg_vocab.stoi['<eos>']:\n",
        "            break\n",
        "\n",
        "    output_tokens = [trg_vocab.itos[i] for i in output_indexes]\n",
        "    return(output_tokens)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n85PqMu2bB0I"
      },
      "source": [
        "def test_manc():\n",
        "    test_list = []\n",
        "    count = 0\n",
        "    test_list = [\"what do you want the thick pens\"]\n",
        "    #, \"she was quite happily pulling it\", \"<unk> these <unk> <unk> are these your water wings\", \"do you think #the policeman'll be after him\", \"that's <unk> that's food for the cat\", \"is it a big hat that\", \"are you #building it up or breaking it up\", \"there were two on there earlier , weren't there\", \"well I can't throw it #now , can I\", \"and what does a cow say\"]\n",
        "    #while len(test_list) < 10:\n",
        "    #    line = tensor_to_line(random.choice(train_data))\n",
        "    #    print(line)\n",
        "    #    if (len(line) > 5\n",
        "    #        and len(line) <10):\n",
        "    #        test_list.append(' '.join(line))\n",
        "    for line in test_list:\n",
        "        print(line)\n",
        "        output = translate_sentence(line, model, 'cuda', 50)\n",
        "        print('input', line)\n",
        "        print('output', output)\n",
        "        count += len(output)\n",
        "    print('word count', count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPT9LEs1f9hd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        },
        "outputId": "e078c551-f6ed-4152-eb03-ded7f238daed"
      },
      "source": [
        "def run_model(name_string):\n",
        "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "    CLIP = 1\n",
        "    print('out_dir', out_dir)\n",
        "    print('hid_dim', HID_DIM)\n",
        "    print('TFR', TFR)\n",
        "    print('epochs', N_EPOCHS)\n",
        "    best_valid_loss = float('inf')\n",
        "    MAX_LEN = 20\n",
        "    max_target = 200\n",
        "    name_string = name_string + str(HID_DIM) + '-50-ep-'\n",
        "    print('name_string', name_string)\n",
        "    for epoch in range(1, N_EPOCHS +1):\n",
        "        part_tr, part_va = [], []\n",
        "        while len(part_tr) < 8000:\n",
        "            part_tr.append(random.choice(train_data))\n",
        "        while len(part_va) < 2000:\n",
        "            part_va.append(random.choice(train_data))\n",
        "        print('tr', part_tr[0])\n",
        "        print('va', part_va[0])\n",
        "        #for i in range(len(part_tr)):\n",
        "        #    if len(part_tr[i][1]) > max_target:\n",
        "        #        part_tr[i] = [part_tr[i][0], truncate_random(part_tr[i][1], max=max_target)]\n",
        "        #        ##part_tr[i][1] = truncate_random(part_tr[i][1], max_target)\n",
        "        #        #print('done')\n",
        "        #        #print('in, out', part_tr[i][0], part_tr[i][1])\n",
        "        #train_iter = DataLoader(part_tr, batch_size=args.batch_size, shuffle=True, num_workers=4, **kwargs)\n",
        "        train_iter = DataLoader(part_tr, batch_size=BATCH_SIZE,\n",
        "                                shuffle=True, collate_fn=generate_batch)\n",
        "        valid_iter = DataLoader(part_va, batch_size=BATCH_SIZE,\n",
        "                                shuffle=True, collate_fn=generate_batch)\n",
        "        #test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
        "        #                       shuffle=True, collate_fn=generate_batch)\n",
        "\n",
        "\n",
        "        start_time = time.time()\n",
        "        train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
        "        valid_loss = evaluate(model, valid_iter, criterion)\n",
        "        del(part_tr)\n",
        "        del(part_va)\n",
        "        del(train_iter)\n",
        "        del(valid_iter)\n",
        "        end_time = time.time()\n",
        "\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        ##print('epoch', epoch+1, epoch_mins, epoch_secs)\n",
        "        print(f'Epoch: {epoch:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "        torch.cuda.empty_cache()\n",
        "        if (epoch > 0\n",
        "                and epoch % 10 == 0):\n",
        "            test_riches(name_string + str(epoch))\n",
        "            test_manc()\n",
        "            torch.save(model.state_dict(), out_dir + name_string + str(epoch) + '.pth')\n",
        "        #if (epoch > 0\n",
        "        #            and epoch % 50 == 0):\n",
        "        #    add_noise()\n",
        "\n",
        "N_EPOCHS = 500\n",
        "\n",
        "PRETRAINED = 'False'\n",
        "for HID_DIM in [500]:\n",
        "    #[250, 500]:\n",
        "        for i in range(1,6):\n",
        "            BATCH_SIZE = 10\n",
        "            print('TFR=', TFR)\n",
        "            print('run is', str(i))\n",
        "            name_string = 'pt-tf05-500-run-' + str(i) +'-wemb-06-07-'\n",
        "            enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT, embeddings_matrix)\n",
        "            ##enc.embedding.weight.data = embeddings_matrix\n",
        "            dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT, embeddings_matrix)\n",
        "            model = Seq2Seq(enc, dec, device).to(device)\n",
        "            model.apply(init_weights)\n",
        "            optimizer = optim.Adam(model.parameters())\n",
        "            load_embs()\n",
        "            print(model.eval())\n",
        "            run_model(name_string)\n"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TFR= 0.5\n",
            "run is 1\n",
            "tensor([[ 0.0557, -0.0476, -0.0189,  0.0130,  0.0096, -0.0216, -0.0460,  0.0618,\n",
            "          0.0727, -0.0695, -0.0156, -0.0057,  0.0717,  0.0491, -0.0563, -0.0414,\n",
            "          0.0372,  0.0319, -0.0511, -0.0439,  0.0589, -0.0223,  0.0388, -0.0289,\n",
            "         -0.0132,  0.0502, -0.0659, -0.0653, -0.0318,  0.0281,  0.0177,  0.0710,\n",
            "          0.0696, -0.0228,  0.0732,  0.0364,  0.0619, -0.0399,  0.0690, -0.0229,\n",
            "          0.0047, -0.0571,  0.0165, -0.0195, -0.0543, -0.0092, -0.0728, -0.0193,\n",
            "         -0.0142, -0.0116,  0.0058,  0.0420, -0.0590,  0.0700, -0.0761, -0.0348,\n",
            "         -0.0762, -0.0546,  0.0341,  0.0635,  0.0665, -0.0771,  0.0501, -0.0280,\n",
            "          0.0139,  0.0601, -0.0043,  0.0219,  0.0460, -0.0049,  0.0098, -0.0747,\n",
            "          0.0350,  0.0086,  0.0164,  0.0254,  0.0480, -0.0496,  0.0529,  0.0339,\n",
            "          0.0686,  0.0528,  0.0551,  0.0549,  0.0478,  0.0627,  0.0386,  0.0170,\n",
            "          0.0610,  0.0445, -0.0339, -0.0415, -0.0024,  0.0107,  0.0693,  0.0696,\n",
            "         -0.0040,  0.0720,  0.0095,  0.0717,  0.0705, -0.0487,  0.0068, -0.0077,\n",
            "         -0.0411, -0.0716, -0.0709, -0.0113,  0.0528,  0.0403,  0.0318,  0.0410,\n",
            "          0.0506,  0.0597,  0.0464, -0.0792, -0.0364, -0.0064,  0.0347, -0.0221,\n",
            "          0.0533, -0.0053,  0.0490,  0.0289, -0.0785, -0.0679, -0.0476,  0.0049,\n",
            "          0.0685,  0.0407,  0.0558, -0.0543, -0.0585, -0.0244, -0.0380, -0.0240,\n",
            "         -0.0756,  0.0199,  0.0520,  0.0309, -0.0208, -0.0537, -0.0182,  0.0702,\n",
            "          0.0402,  0.0338,  0.0612, -0.0533,  0.0038, -0.0447,  0.0227, -0.0741,\n",
            "         -0.0464, -0.0693,  0.0462,  0.0370,  0.0229,  0.0667, -0.0013, -0.0712,\n",
            "          0.0115, -0.0679, -0.0594,  0.0237, -0.0341, -0.0461,  0.0233, -0.0307,\n",
            "         -0.0513, -0.0260, -0.0742,  0.0075, -0.0040, -0.0254,  0.0798,  0.0376,\n",
            "          0.0036,  0.0396,  0.0639, -0.0489,  0.0140,  0.0319,  0.0284, -0.0443,\n",
            "          0.0222,  0.0745, -0.0036,  0.0272, -0.0747, -0.0703, -0.0455, -0.0143,\n",
            "         -0.0322,  0.0795, -0.0716,  0.0022,  0.0002,  0.0608,  0.0684,  0.0309,\n",
            "         -0.0275, -0.0537, -0.0643,  0.0577,  0.0645,  0.0797,  0.0182,  0.0679,\n",
            "         -0.0300,  0.0613, -0.0467,  0.0236,  0.0071,  0.0325, -0.0451,  0.0586,\n",
            "          0.0543, -0.0409,  0.0103,  0.0328, -0.0568, -0.0576,  0.0291, -0.0646,\n",
            "         -0.0628, -0.0164, -0.0148,  0.0582, -0.0772, -0.0103, -0.0341,  0.0556,\n",
            "         -0.0494,  0.0126, -0.0480,  0.0555, -0.0758,  0.0294,  0.0097,  0.0312,\n",
            "         -0.0211,  0.0339,  0.0784, -0.0044, -0.0515,  0.0770,  0.0347, -0.0741,\n",
            "         -0.0207,  0.0297, -0.0771,  0.0099,  0.0268,  0.0171,  0.0795,  0.0777]])\n",
            "10359\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-03bd0c0bc933>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mload_embs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-98-7d5dd4d2465c>\u001b[0m in \u001b[0;36mload_embs\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'wants'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wants'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                         \u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'wants'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'enc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (256) must match the existing size (50) at non-singleton dimension 0.  Target sizes: [256].  Tensor sizes: [50]"
          ]
        }
      ]
    }
  ]
}